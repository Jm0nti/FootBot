import os
import uuid
import sqlite3
from pathlib import Path
from typing import TypedDict, List, Union, Literal
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.tools import tool
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import pandas as pd
import requests
from dotenv import load_dotenv
from tavily import TavilyClient
import base64

# Cargar variables de entorno
load_dotenv()

# Logger local
import logging
logger = logging.getLogger(__name__)

# --- 1. CONFIGURACI√ìN DE MODELOS ---
llm_fast = ChatGroq(temperature=0, model_name="openai/gpt-oss-20b")
llm_smart = ChatGoogleGenerativeAI(model="gemini-2.0-flash-lite", temperature=0)

# --- 2. DEFINICI√ìN DE HERRAMIENTAS ---

@tool
def sql_executor(query: str) -> str:
    """
    Ejecuta consultas SQL sobre una base de datos SQLite con estad√≠sticas de f√∫tbol.
    Args:
        query: Consulta SQL a ejecutar (SELECT statements)
    Returns:
        Resultados de la consulta en formato texto
    """
    try:
        logger.info("[sql_executor] Ejecutando consulta SQL: %s", query)
        # Ruta a la base de datos (ajustar seg√∫n tu estructura)
        db_path = "data/soccer_stats.db"
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Validar que sea SELECT
        if not query.strip().upper().startswith("SELECT"):
            return "Error: Solo se permiten consultas SELECT por seguridad."
        
        cursor.execute(query)
        results = cursor.fetchall()
        columns = [description[0] for description in cursor.description]
        conn.close()
        
        if not results:
            logger.info("[sql_executor] No se encontraron resultados para la consulta")
            return "No se encontraron resultados para esta consulta."
        
        # Formatear resultados
        df = pd.DataFrame(results, columns=columns)
        logger.info("[sql_executor] Resultado rows=%d cols=%d", len(df), len(df.columns))
        return f"Resultados de la consulta:\n{df.to_string(index=False)}"
    
    except Exception as e:
        logger.exception("[sql_executor] Error al ejecutar la consulta SQL: %s", e)
        return f"Error al ejecutar la consulta SQL: {str(e)}"


@tool
def faiss_retriever(query: str) -> str:
    """
    Busca informaci√≥n en la base de conocimiento vectorial (FAISS) sobre equipos,
    clubes, competencias, historia del f√∫tbol y reglamentos.
    Usa OpenAI text-embedding-3-small para generar embeddings de alta calidad.
    Args:
        query: Pregunta o t√©rmino de b√∫squeda
    Returns:
        Contexto relevante recuperado de los documentos
    """
    try:
        logger.info("[faiss_retriever] Buscando en FAISS: %s", query)
        # Validar que existe la API key de OpenAI
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            logger.warning("[faiss_retriever] OPENAI_API_KEY no configurada")
            return "Error: OPENAI_API_KEY no est√° configurada en el archivo .env"
        
        vector_store_path = "data/faiss_index"
        
        # Inicializar embeddings de OpenAI con text-embedding-3-small
        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=openai_api_key
        )
        
        # Cargar el √≠ndice existente
        vectorstore = FAISS.load_local(
            vector_store_path, 
            embeddings, 
            allow_dangerous_deserialization=True
        )
        
        # Realizar b√∫squeda de similitud (k=5 para obtener los 5 documentos m√°s relevantes)
        docs = vectorstore.similarity_search(query, k=5)
        
        if not docs:
            logger.info("[faiss_retriever] No se encontraron docs para la query")
            return "No se encontr√≥ informaci√≥n relevante en la base de conocimiento."
        
        # Formatear el contexto recuperado
        context = "\n\n".join([f"- {doc.page_content}" for doc in docs])
        logger.info("[faiss_retriever] Documentos recuperados: %d", len(docs))
        
        return f"Contexto relevante encontrado:\n{context}"
    
    except Exception as e:
        logger.exception("[faiss_retriever] Error al buscar en la base de conocimiento: %s", e)
        return f"Error al buscar en la base de conocimiento: {str(e)}"



PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")

# Endpoint de Perplexity
PERPLEXITY_API_URL = "https://api.perplexity.ai/chat/completions"


# ============================================================================
# TOOL: WEB SEARCH CON PERPLEXITY
# ============================================================================

@tool
def web_search_tool(query: str) -> str:
    """
    Realiza una b√∫squeda en internet sobre f√∫tbol usando Perplexity AI para 
    obtener informaci√≥n actualizada, precisa y con fuentes verificadas sobre 
    partidos, noticias, resultados y eventos recientes.
    
    Args:
        query: T√©rmino de b√∫squeda relacionado con f√∫tbol (equipos, partidos, noticias)
    
    Returns:
        Respuesta detallada con informaci√≥n actualizada y fuentes citadas
    
    Ejemplos de uso:
        - "¬øCu√°ndo juega el Real Madrid pr√≥ximamente?"
        - "√öltimas noticias del FC Barcelona"
        - "Resultados de LaLiga de hoy"
    """
    try:
        # Construir el prompt optimizado para b√∫squedas de f√∫tbol
        search_prompt = f"""Busca informaci√≥n actualizada sobre: {query}

Proporciona:
1. Informaci√≥n espec√≠fica y verificada
2. Fechas y detalles concretos si est√°n disponibles
3. Las fuentes de donde obtuviste la informaci√≥n

Mant√©n la respuesta concisa pero informativa."""

        # Headers para la API de Perplexity
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        # Payload para la API
        # Usando modelo sonar-pro para mejor precisi√≥n en b√∫squedas
        payload = {
            "model": "sonar-pro",  # Mejor modelo para b√∫squedas web
            "messages": [
                {
                    "role": "system",
                    "content": "Eres un asistente experto en b√∫squedas de informaci√≥n deportiva, especialmente f√∫tbol. Proporciona informaci√≥n precisa, actualizada y con fuentes verificables."
                },
                {
                    "role": "user",
                    "content": search_prompt
                }
            ],
            "temperature": 0.2,  # Baja temperatura para respuestas m√°s precisas
            "top_p": 0.9,
            "return_citations": True,  # Importante: incluir citas
            "search_recency_filter": "month",  # Priorizar resultados del √∫ltimo mes
            "stream": False
        }
        
        # Realizar request a Perplexity API
        response = requests.post(
            PERPLEXITY_API_URL,
            headers=headers,
            json=payload,
            timeout=30  # Timeout de 30 segundos
        )
        
        # Verificar si la request fue exitosa
        response.raise_for_status()
        
        # Parsear respuesta
        data = response.json()
        
        # Extraer contenido y citas
        content = data['choices'][0]['message']['content']
        citations = data.get('citations', [])
        
        # Construir respuesta estructurada
        search_summary = f"üîç **B√∫squeda: '{query}'**\n\n"
        search_summary += f"{content}\n\n"
        
        # Agregar citas si existen
        if citations:
            search_summary += "üìö **Fuentes consultadas:**\n"
            for idx, citation in enumerate(citations[:5], 1):  # M√°ximo 5 fuentes
                search_summary += f"{idx}. {citation}\n"
        
        return search_summary
    
    except requests.exceptions.HTTPError as e:
        # Error HTTP espec√≠fico
        status_code = e.response.status_code
        if status_code == 401:
            return "‚ùå Error de autenticaci√≥n: Verifica tu API key de Perplexity."
        elif status_code == 429:
            return "‚ùå L√≠mite de rate exceeded. Espera un momento e intenta de nuevo."
        else:
            return f"‚ùå Error HTTP {status_code}: {str(e)}"
    
    except requests.exceptions.Timeout:
        return "‚ùå Timeout: La b√∫squeda tard√≥ demasiado. Intenta con una query m√°s espec√≠fica."
    
    except requests.exceptions.RequestException as e:
        return f"‚ùå Error de conexi√≥n: {str(e)}\nVerifica tu conexi√≥n a internet."
    
    except Exception as e:
        return f"‚ùå Error inesperado al realizar b√∫squeda: {str(e)}"



@tool
def formation_image_tool(team_name: str) -> dict:
    """
    Obtiene la imagen de formaci√≥n t√°ctica de un equipo espec√≠fico.
    
    Args:
        team_name: Nombre del equipo (ej: "Barcelona", "Real Madrid")
    
    Returns:
        Diccionario con:
        - image_url: Ruta de la imagen (para web)
        - image_base64: Imagen codificada en base64 (para env√≠o)
        - text: Texto descriptivo
        - type: "formation"
        - team_name: Nombre del equipo
    """
    try:
        logger.info("[formation_image_tool] Buscando formaci√≥n para: %s", team_name)
        formations_dir = Path("assets/formations")
        
        # Normalizar nombre del equipo
        team_clean = team_name.strip().lower().replace(" ", "_")
        
        # Buscar archivo de formaci√≥n (m√∫ltiples variantes)
        possible_files = [
            f"{team_clean}_formation.png",
            f"{team_clean}_Formation.png",
            f"{team_clean}.png",
            f"{team_name.strip().replace(' ', '_')}_formation.png",
            f"{team_name.strip().replace(' ', '_')}.png"
        ]
        
        found_file = None
        for filename in possible_files:
            file_path = formations_dir / filename
            if file_path.exists():
                found_file = file_path
                logger.info("[formation_image_tool] Imagen encontrada: %s", filename)
                break
        
        # Si se encontr√≥ la imagen
        if found_file:
            # Leer imagen y convertir a base64
            with open(found_file, 'rb') as img_file:
                image_data = img_file.read()
                image_base64 = base64.b64encode(image_data).decode('utf-8')
            
            return {
                "image_url": f"/assets/formations/{found_file.name}",
                "image_base64": image_base64,
                "text": f"üìã Formaci√≥n t√°ctica del {team_name}",
                "type": "formation",
                "team_name": team_name,
                "success": True
            }
        
        # Si no existe la imagen
        logger.warning("[formation_image_tool] No se encontr√≥ imagen para: %s", team_name)
        logger.warning("[formation_image_tool] Archivos buscados: %s", possible_files)
        
        return {
            "image_url": None,
            "image_base64": None,
            "text": f"‚ö†Ô∏è No se encontr√≥ la formaci√≥n t√°ctica para {team_name}. Verifica que exista el archivo en assets/formations/",
            "type": "formation",
            "team_name": team_name,
            "success": False
        }
    
    except Exception as e:
        logger.exception("[formation_image_tool] Error procesando formaci√≥n: %s", e)
        return {
            "image_url": None,
            "image_base64": None,
            "text": f"‚ùå Error al obtener la formaci√≥n: {str(e)}",
            "type": "formation",
            "team_name": team_name,
            "success": False,
            "error": str(e)
        }



# --- 3. ESTADO DEL GRAFO ---
class AgentState(TypedDict):
    messages: List[Union[HumanMessage, AIMessage, SystemMessage]]
    next_step: str
    classification: str
    needs_critic: bool
    formation_data: dict | None
    trace: List[str]


# --- 4. NODOS DEL GRAFO ---

def classifier_node(state: AgentState) -> dict:
    """Clasifica la intenci√≥n del usuario en uno de los 5 agentes"""
    last_msg = state['messages'][-1].content
    state.setdefault('trace', []).append('classifier')
    logger.info("[classifier] Entrada. Mensaje: %s", last_msg)

    system_prompt = """Eres un clasificador experto. Analiza la pregunta del usuario y clasifica en UNA de estas categor√≠as:

1. 'identity' - Si pregunta sobre ti, tus capacidades, qu√© haces, qui√©n eres, o un saludo general
2. 'formation' - Si pide ver la formaci√≥n t√°ctica de un equipo (ej: "muestra la formaci√≥n del Barcelona, cual es la formaci√≥n del Real Madrid, dame el 11 titular del liverpool")
3. 'sql_stats' - Si pide estad√≠sticas, n√∫meros, goles, asistencias, comparaciones num√©ricas
4. 'rag_knowledge' - Si pregunta sobre historia, biograf√≠as, reglamentos, fundaci√≥n de clubes
5. 'web_search' - Si pregunta sobre noticias recientes, partidos de hoy/ayer, eventos actuales, o preguntas que se salen de tu base de conocimiento que est√©n el el dominio del f√∫tbol

Responde SOLO con una de estas palabras: identity, formation, sql_stats, rag_knowledge, web_search"""
    
    response = llm_fast.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"Pregunta del usuario: {last_msg}")
    ])
    
    classification = response.content.strip().lower()
    
    # Validar clasificaci√≥n
    valid_classifications = ['identity', 'formation', 'sql_stats', 'rag_knowledge', 'web_search']
    if classification not in valid_classifications:
        classification = 'identity'
    logger.info("[classifier] Clasificado como: %s", classification)
    return {
        "classification": classification,
        "next_step": classification,
        "trace": state.get('trace')
    }


def identity_node(state: AgentState) -> dict:
    """Responde preguntas sobre la identidad y capacidades del sistema"""
    identity_prompt = """Eres un asistente especializado en f√∫tbol. Tus capacidades incluyen:

- Responder preguntas sobre estad√≠sticas de jugadores y equipos
- Proporcionar informaci√≥n hist√≥rica sobre clubes y competencias
- Mostrar formaciones t√°cticas de equipos
- Buscar noticias recientes y resultados actuales
- Explicar reglas y reglamentos del f√∫tbol

Responde de manera amigable y concisa sobre tus capacidades."""
    
    state.setdefault('trace', []).append('identity')
    logger.info("[identity] Respondiendo a identidad/capacidades")
    messages = [SystemMessage(content=identity_prompt)] + state['messages']
    response = llm_fast.invoke(messages)
    
    return {
        "messages": state['messages'] + [response],
        "needs_critic": False,
        "next_step": "end",
        "trace": state.get('trace')
    }


def formation_node(state: dict[str, any]) -> dict:
    """
    Maneja solicitudes de formaciones t√°cticas.
    
    Flujo:
    1. Extrae el nombre del equipo de la pregunta del usuario
    2. Busca la imagen de formaci√≥n usando formation_image_tool
    3. Prepara la respuesta con la imagen (si existe)
    4. Retorna el estado actualizado
    
    Args:
        state: Estado del agente (AgentState) con:
            - messages: Lista de mensajes
            - trace: Lista de pasos ejecutados
            - formation_data: Datos de la formaci√≥n (se agregar√°)
    
    Returns:
        Estado actualizado con:
        - messages: Mensajes + respuesta del agente
        - formation_data: Datos de la imagen de formaci√≥n
        - needs_critic: False (no necesita validaci√≥n)
        - next_step: "end" (termina el flujo)
        - trace: Traza actualizada
    """
    last_msg = state['messages'][-1].content
    logger.info("[formation_node] Procesando solicitud de formaci√≥n")
    
    # Inicializar trace si no existe
    state.setdefault('trace', []).append('formation')
    
    # ========== PASO 1: EXTRAER NOMBRE DEL EQUIPO ==========
    extraction_prompt = f"""Analiza esta pregunta y extrae SOLAMENTE el nombre del equipo.

Pregunta: {last_msg}

INSTRUCCIONES:
- Devuelve SOLO el nombre del equipo, sin explicaciones
- Usa el nombre oficial com√∫n (ej: "Barcelona" no "FC Barcelona")
- Si hay m√∫ltiples equipos, devuelve el primero mencionado
- Si no hay equipo claro, devuelve "No especificado"

Ejemplos:
- "Mu√©strame la alineaci√≥n del Barcelona" ‚Üí Barcelona
- "Formaci√≥n del Real Madrid" ‚Üí Real Madrid
- "¬øCu√°l es el 11 inicial del PSG?" ‚Üí PSG

Equipo:"""
    
    try:
        team_extraction = llm_fast.invoke([HumanMessage(content=extraction_prompt)])
        team_name = team_extraction.content.strip()
        logger.info("[formation_node] Equipo extra√≠do: '%s'", team_name)
        
        # Validar que se extrajo un equipo
        if not team_name or team_name.lower() in ["no especificado", "ninguno", "no hay"]:
            logger.warning("[formation_node] No se pudo extraer un equipo v√°lido")
            return {
                "messages": state['messages'] + [AIMessage(
                    content="‚ö†Ô∏è No pude identificar el equipo del que quieres ver la formaci√≥n. Por favor, especifica el nombre del equipo."
                )],
                "formation_data": None,
                "needs_critic": False,
                "next_step": "end",
                "trace": state.get('trace')
            }
    
    except Exception as e:
        logger.exception("[formation_node] Error extrayendo nombre del equipo: %s", e)
        return {
            "messages": state['messages'] + [AIMessage(
                content=f"‚ùå Error al procesar tu solicitud: {str(e)}"
            )],
            "formation_data": None,
            "needs_critic": False,
            "next_step": "end",
            "trace": state.get('trace')
        }
    
    # ========== PASO 2: OBTENER IMAGEN DE FORMACI√ìN ==========
    try:
        logger.info("[formation_node] Invocando formation_image_tool para: %s", team_name)
        formation_result = formation_image_tool.invoke({"team_name": team_name})
        
    except Exception as e:
        logger.exception("[formation_node] Error invocando formation_image_tool: %s", e)
        formation_result = {
            "image_url": None,
            "image_base64": None,
            "text": f"‚ùå Error al obtener la formaci√≥n: {str(e)}",
            "type": "formation",
            "team_name": team_name,
            "success": False
        }
    
    # ========== PASO 3: PREPARAR RESPUESTA ==========
    response_text = formation_result.get('text', '')
    success = formation_result.get('success', False)
    
    logger.info("[formation_node] Resultado - Success: %s, Text: %s", success, response_text)
    
    # Si hay imagen, agregar informaci√≥n adicional
    if success and formation_result.get('image_url'):
        response_text += "\n\nüí° Puedes ver la formaci√≥n t√°ctica en la imagen mostrada arriba."
    
    # ========== PASO 4: RETORNAR ESTADO ACTUALIZADO ==========
    return {
        "messages": state['messages'] + [AIMessage(content=response_text)],
        "formation_data": formation_result,  # IMPORTANTE: Esto se usa en el servidor
        "needs_critic": False,
        "next_step": "end",
        "trace": state.get('trace')
    }


def sql_agent_node(state: AgentState) -> dict:
    """Agente que maneja consultas de estad√≠sticas con SQL"""
    system_prompt = """Eres un experto en estad√≠sticas de f√∫tbol. Tienes acceso a una base de datos SQL.

Tablas disponibles:
- player_stats (player_name, season, goals, assists, team)

Cuando el usuario pida estad√≠sticas:
1. Genera una consulta SQL SELECT apropiada
2. Usa la herramienta sql_executor para ejecutarla
3. Interpreta los resultados y responde al usuario

Si no puedes responder con SQL, dilo claramente."""
    
    state.setdefault('trace', []).append('sql_agent')
    logger.info("[sql_agent] Iniciando SQL agent con mensaje: %s", state['messages'][-1].content)
    messages = [SystemMessage(content=system_prompt)] + state['messages']
    
    # Vincular herramienta SQL
    llm_with_tools = llm_smart.bind_tools([sql_executor])
    response = llm_with_tools.invoke(messages)
    
    # Si hay tool calls, ejecutarlos
    if hasattr(response, 'tool_calls') and response.tool_calls:
        messages_with_response = messages + [response]
        
        for tool_call in response.tool_calls:
            logger.info("[sql_agent] Ejecutando tool_call: %s", tool_call)
            try:
                tool_result = sql_executor.invoke(tool_call['args'])
            except Exception as e:
                logger.exception("[sql_agent] Error ejecutando sql_executor: %s", e)
                tool_result = f"Error ejecutando SQL: {e}"
            messages_with_response.append(
                AIMessage(content=f"Resultado de SQL: {tool_result}")
            )
        
        # Generar respuesta final
        final_response = llm_smart.invoke(messages_with_response)
        
        return {
            "messages": state['messages'] + [final_response],
            "needs_critic": True,
            "next_step": "critic",
            "trace": state.get('trace')
        }
    
    return {
        "messages": state['messages'] + [response],
        "needs_critic": True,
        "next_step": "critic",
        "trace": state.get('trace')
    }


def rag_agent_node(state: AgentState) -> dict:
    """Agente que usa RAG para responder sobre historia y conocimiento general"""
    system_prompt = """Eres un experto en historia del f√∫tbol, biograf√≠as y reglamentos.
Tienes acceso a una base de conocimiento vectorial. 

Cuando el usuario pregunte sobre historia, clubes, o reglas:
1. Usa la herramienta faiss_retriever para buscar contexto relevante
2. Basa tu respuesta en el contexto recuperado
3. Si no encuentras informaci√≥n, ind√≠calo claramente"""
    
    state.setdefault('trace', []).append('rag_agent')
    logger.info("[rag_agent] Ejecutando RAG con mensaje: %s", state['messages'][-1].content)
    messages = [SystemMessage(content=system_prompt)] + state['messages']
    
    llm_with_tools = llm_smart.bind_tools([faiss_retriever])
    response = llm_with_tools.invoke(messages)
    
    # Si hay tool calls, ejecutarlos
    if hasattr(response, 'tool_calls') and response.tool_calls:
        messages_with_response = messages + [response]
        
        for tool_call in response.tool_calls:
            logger.info("[rag_agent] Ejecutando tool_call: %s", tool_call)
            try:
                tool_result = faiss_retriever.invoke(tool_call['args'])
            except Exception as e:
                logger.exception("[rag_agent] Error ejecutando faiss_retriever: %s", e)
                tool_result = f"Error recuperando contexto: {e}"
            messages_with_response.append(
                AIMessage(content=f"Contexto recuperado: {tool_result}")
            )
        
        final_response = llm_smart.invoke(messages_with_response)
        
        return {
            "messages": state['messages'] + [final_response],
            "needs_critic": True,
            "next_step": "critic",
            "trace": state.get('trace')
        }
    
    return {
        "messages": state['messages'] + [response],
        "needs_critic": True,
        "next_step": "critic",
        "trace": state.get('trace')
    }





def web_search_node(state: dict) -> dict:
    """
    Nodo del agente que busca informaci√≥n actual en la web sobre f√∫tbol usando Perplexity.
    
    Este agente:
    1. Recibe la pregunta del usuario
    2. Decide si necesita usar web_search_tool
    3. Ejecuta la b√∫squeda si es necesario
    4. Genera una respuesta natural basada en los resultados
    5. Pasa al nodo cr√≠tico para validaci√≥n
    
    Args:
        state: Estado del agente con estructura AgentState
            - messages: Lista de mensajes de la conversaci√≥n
            - needs_critic: Bool indicando si pasa por validaci√≥n
            - next_step: Siguiente nodo en el grafo
    
    Returns:
        Estado actualizado con respuesta del agente
    """
    
    # System prompt que define el comportamiento del agente
    system_prompt = """Eres un experto en noticias y eventos actuales de f√∫tbol.
Tienes acceso a b√∫squeda web en tiempo real usando Perplexity AI para informaci√≥n precisa y actualizada.

INSTRUCCIONES:
1. Cuando el usuario pregunte sobre eventos recientes, partidos, noticias o informaci√≥n actualizada:
   - USA la herramienta web_search_tool para buscar
   - Construye queries de b√∫squeda claras y espec√≠ficas en espa√±ol
   - Incluye contexto relevante en la query (fechas, competiciones, etc.)
   
2. Al recibir resultados de b√∫squeda:
   - Resume la informaci√≥n de manera clara y concisa
   - Destaca los datos m√°s importantes (fechas, resultados, nombres)
   - Menciona que la informaci√≥n proviene de fuentes actualizadas
   
3. S√© natural y conversacional en tus respuestas
4. Si no encuentras informaci√≥n espec√≠fica, sugiere alternativas o adm√≠telo honestamente

EJEMPLOS DE QUERIES EFECTIVAS:
- Usuario: "¬øCu√°ndo juega el Real Madrid?" 
  ‚Üí Query: "Real Madrid pr√≥ximo partido fecha horario 2024"
  
- Usuario: "√öltimas noticias del Barcelona"
  ‚Üí Query: "FC Barcelona noticias √∫ltimas fichajes resultados"
  
- Usuario: "¬øQui√©n gan√≥ ayer en LaLiga?"
  ‚Üí Query: "LaLiga resultados partido ayer marcador"

IMPORTANTE: Perplexity proporciona informaci√≥n muy precisa, conf√≠a en sus resultados.
"""
    
    # Construir mensajes para el LLM
    messages = [SystemMessage(content=system_prompt)] + state['messages']
    
    # Vincular la tool al LLM (permite que el LLM decida cu√°ndo usarla)
    llm_with_tools = llm_smart.bind_tools([web_search_tool])
    
    # Primera invocaci√≥n: LLM decide si usar la tool
    response = llm_with_tools.invoke(messages)
    
    # CASO 1: El LLM decidi√≥ usar la tool
    if hasattr(response, 'tool_calls') and response.tool_calls:
        # Agregar la respuesta del LLM con tool_calls al historial
        messages_with_response = messages + [response]
        
        # Ejecutar cada tool call solicitada
        for tool_call in response.tool_calls:
            try:
                # Invocar la tool con los argumentos que el LLM proporcion√≥
                tool_result = web_search_tool.invoke(tool_call['args'])
                
                # CR√çTICO: Usar ToolMessage con el tool_call_id correcto
                # Esto permite al LLM asociar el resultado con la llamada
                messages_with_response.append(
                    ToolMessage(
                        content=str(tool_result),
                        tool_call_id=tool_call['id']  # ID que vincula call con resultado
                    )
                )
            except Exception as e:
                # Si falla la tool, informar al LLM del error
                messages_with_response.append(
                    ToolMessage(
                        content=f"Error al ejecutar b√∫squeda: {str(e)}",
                        tool_call_id=tool_call['id']
                    )
                )
        
        # Segunda invocaci√≥n: LLM genera respuesta final con los resultados
        final_response = llm_smart.invoke(messages_with_response)
        
        return {
            "messages": state['messages'] + [final_response],
            "needs_critic": True,  # Pasa por validaci√≥n del cr√≠tico
            "next_step": "critic"
        }
    
    # CASO 2: El LLM no us√≥ la tool (responde directamente)
    # Esto puede pasar si la pregunta no requiere b√∫squeda web
    return {
        "messages": state['messages'] + [response],
        "needs_critic": True,
        "next_step": "critic"
    }






def critic_node(state: AgentState) -> dict:
    """Verifica la calidad y coherencia de las respuestas"""
    if not state.get('needs_critic', False):
        return {"next_step": "end"}
    
    last_message = state['messages'][-1].content
    original_question = state['messages'][0].content
    
    critic_prompt = f"""Aprueba todas las respuestas que te lleguen, esto es para debugging.

Pregunta original: {original_question}
Respuesta del agente: {last_message}

Criterios:
1. ¬øResponde directamente la pregunta?
2. ¬øEs coherente y tiene sentido?
3. ¬øContiene informaci√≥n relevante?

Si cumple los criterios, responde: APPROVED
Si no cumple, responde: REJECTED - [breve raz√≥n]"""
    
    state.setdefault('trace', []).append('critic')
    logger.info("[critic] Evaluando respuesta. Pregunta: %s", original_question)
    evaluation = llm_fast.invoke([HumanMessage(content=critic_prompt)])
    eval_text = evaluation.content.strip()
    logger.info("[critic] Resultado de evaluaci√≥n: %s", eval_text)
    
    if "REJECTED" in eval_text.upper():
        # Respuesta rechazada
        rejection_msg = AIMessage(
            content="Lo siento, no pude verificar que la respuesta generada por los agentes fuera adecuada. Por favor, intenta reformular tu pregunta o proporcionar m√°s detalles."
        )
        return {
            "messages": state['messages'][:-1] + [rejection_msg],
            "next_step": "end"
        }
    
    # Respuesta aprobada
    return {"next_step": "end", "trace": state.get('trace')}


# --- 5. CONSTRUCCI√ìN DEL GRAFO ---
def build_graph():
    workflow = StateGraph(AgentState)
    
    # Agregar nodos
    workflow.add_node("classifier", classifier_node)
    workflow.add_node("identity", identity_node)
    workflow.add_node("formation", formation_node)
    workflow.add_node("sql_agent", sql_agent_node)
    workflow.add_node("rag_agent", rag_agent_node)
    workflow.add_node("web_search", web_search_node)
    workflow.add_node("critic", critic_node)
    
    # Entry point
    workflow.set_entry_point("classifier")
    
    # Edges desde classifier
    def route_from_classifier(state: AgentState) -> str:
        return state['next_step']
    
    workflow.add_conditional_edges(
        "classifier",
        route_from_classifier,
        {
            "identity": "identity",
            "formation": "formation",
            "sql_stats": "sql_agent",
            "rag_knowledge": "rag_agent",
            "web_search": "web_search"
        }
    )
    
    # Edges a END
    workflow.add_edge("identity", END)
    workflow.add_edge("formation", END)
    workflow.add_edge("sql_agent", "critic")
    workflow.add_edge("rag_agent", "critic")
    workflow.add_edge("web_search", "critic")
    workflow.add_edge("critic", END)
    
    return workflow.compile(checkpointer=MemorySaver())


# --- 6. CLASE WRAPPER ---
class SoccerBot:
    def __init__(self):
        self.thread_id = str(uuid.uuid4())
        self.graph = build_graph()
        self._interaction_count = 0
        self._interaction_limit = 50
    
    def ask(self, message: str) -> dict:
        """M√©todo principal que procesa preguntas del usuario"""
        config = {"configurable": {"thread_id": self.thread_id}}
        
        try:
            logger.info("[SoccerBot.ask] Invocando grafo con mensaje: %s", message)
            # Ejecutar el grafo
            final_state = self.graph.invoke(
                {
                    "messages": [HumanMessage(content=message)],
                    "next_step": "",
                    "classification": "",
                    "needs_critic": False,
                    "formation_data": None,
                    "trace": []
                },
                config=config
            )
            
            # Extraer respuesta
            last_message = final_state['messages'][-1]
            response_text = last_message.content
            
            # Verificar si hay datos de formaci√≥n
            image_data = None
            if final_state.get('formation_data'):
                image_data = final_state['formation_data'].get('image_url')
            
            self._interaction_count += 1
            trace = final_state.get('trace') if isinstance(final_state, dict) else None
            logger.info("[SoccerBot.ask] Respuesta generada. agent=%s, trace=%s", final_state.get('classification', 'unknown'), trace)
            
            return {
                "answer": response_text,
                "image": image_data,
                "agent_used": final_state.get('classification', 'unknown'),
                "trace": trace
            }
        
        except Exception as e:
            logger.exception("[SoccerBot.ask] Error procesando solicitud: %s", e)
            return {
                "answer": f"Error al procesar la solicitud: {str(e)}",
                "image": None,
                "agent_used": "error",
                "trace": []
            }
    
    def clear_memory(self):
        """Reinicia la conversaci√≥n"""
        self.thread_id = str(uuid.uuid4())
        self._interaction_count = 0