"""
FAISS_Creator Agent - Preprocesamiento de Documentos y Creaci√≥n de Vector DBs
==================================================================================

Este agente se ejecuta de forma independiente para:
1. Resumir documentos .txt usando Groq LLM (SALTA RES√öMENES EXISTENTES)
2. Crear 3 bases de datos vectoriales FAISS categorizadas usando embeddings de HuggingFace (gratuitos)
3. Preparar el conocimiento para el sistema principal

NO forma parte del flujo conversacional, solo prepara los datos.
SOLO REQUIERE: GROQ_API_KEY

v2.1 - Detecta y salta autom√°ticamente res√∫menes ya existentes
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Literal, Tuple
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_huggingface import HuggingFaceEmbeddings  # Embeddings GRATUITOS
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from dotenv import load_dotenv
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
current_dir = Path(__file__).resolve().parent  # bots/
app_dir = current_dir.parent                    # app/

# El .env est√° en app/ directamente
env_path = app_dir / ".env"

if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    logger.info(f"üìÅ .env encontrado en: {env_path}")
else:
    logger.error(f"‚ùå .env NO encontrado en: {env_path}")

# Validar SOLO Groq API key
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY:
    error_msg = f"""
‚ùå ERROR: GROQ_API_KEY no encontrada.

üîß Ubicaciones verificadas:
   - {env_path}

üìù Tu .env debe contener:
   GROQ_API_KEY=gsk_tu_key_real_aqui

üåê Obt√©n tu key en: https://console.groq.com/keys
"""
    raise ValueError(error_msg)

logger.info(f"‚úÖ GROQ_API_KEY cargada (primeros 10 chars: {GROQ_API_KEY[:10]}...)")


# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

class FAISS_Creator:
    """Configuraci√≥n centralizada del FAISS_Creator"""

    SCRIPT_PATH = Path(__file__).resolve()
    SCRIPT_DIR = SCRIPT_PATH.parent           # bots/
    APP_DIR = SCRIPT_DIR.parent               # app/
    DATA_DIR = APP_DIR / "data"               # app/data/

    SUMMARIES_DIR = DATA_DIR / "summaries"
    VECTOR_DBS_DIR = DATA_DIR / "vector_stores"

    # Subdirectorios de documentos originales
    PLAYERS_DIR = DATA_DIR / "biografias_jugadores"
    TEAMS_DIR = DATA_DIR / "informacion_equipos"
    RULES_DIR = DATA_DIR / "competiciones_y_reglas"

    # Subdirectorios de summaries
    PLAYERS_SUMMARIES = SUMMARIES_DIR / "biografias_jugadores"
    TEAMS_SUMMARIES = SUMMARIES_DIR / "informacion_equipos"
    RULES_SUMMARIES = SUMMARIES_DIR / "competiciones_y_reglas"

    # Vector DBs paths
    PLAYERS_VECTORDB = VECTOR_DBS_DIR / "jugadores_faiss"
    TEAMS_VECTORDB = VECTOR_DBS_DIR / "equipos_faiss"
    RULES_VECTORDB = VECTOR_DBS_DIR / "reglas_faiss"

    # Modelos

    GROQ_MODEL = "openai/gpt-oss-20b" #Para textos cortos

    # Embeddings de HuggingFace (GRATUITOS, no requieren API key)
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

    # Chunking
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    MAX_CHARS_PER_CHUNK = 900000000
    CHUNK_OVERLAP_CHARS = 1000

    @classmethod
    def create_directories(cls):
        """Crea solo los directorios que no existen (summaries y vector_stores)"""
        dirs = [
            cls.SUMMARIES_DIR,
            cls.VECTOR_DBS_DIR,
            cls.PLAYERS_SUMMARIES,
            cls.TEAMS_SUMMARIES,
            cls.RULES_SUMMARIES,
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
        logger.info("‚úÖ Estructura de directorios de salida creada")

        # Verificar que existan los directorios de entrada
        logger.info(f"\nüìÇ Verificando directorios de entrada:")
        logger.info(f"   Data dir: {cls.DATA_DIR}")

        missing_dirs = []
        for name, dir_path in [
            ("biografias_jugadores", cls.PLAYERS_DIR),
            ("informacion_equipos", cls.TEAMS_DIR),
            ("competiciones_y_reglas", cls.RULES_DIR)
        ]:
            if dir_path.exists():
                txt_count = len(list(dir_path.glob("*.txt")))
                logger.info(f"   ‚úÖ {name}: {txt_count} archivos .txt")
            else:
                logger.warning(f"   ‚ùå {name}: NO EXISTE - {dir_path}")
                missing_dirs.append(str(dir_path))

        if missing_dirs:
            logger.warning(f"\n‚ö†Ô∏è ADVERTENCIA: Faltan carpetas con datos")
            logger.warning(f"Aseg√∫rate de que existan estas carpetas:")
            for d in missing_dirs:
                logger.warning(f"  - {d}")


# ============================================================================
# TOOL 1: DOCUMENT SUMMARIZER (CON DETECCI√ìN DE RES√öMENES EXISTENTES)
# ============================================================================

class DocumentSummarizer:
    """
    Tool que resume documentos .txt usando Groq LLM.

    üÜï v2.1 Caracter√≠sticas:
    - Detecta autom√°ticamente res√∫menes existentes y los salta
    - Solo procesa documentos nuevos o faltantes
    - Modo 'force' para regenerar todos los res√∫menes
    - Estad√≠sticas detalladas: generados, saltados, errores
    """

    def __init__(self):
        self.llm = ChatGroq(
            temperature=0.3,
            model_name=FAISS_Creator.GROQ_MODEL,
            api_key=GROQ_API_KEY
        )
        logger.info(f"[DocumentSummarizer] Inicializado con {FAISS_Creator.GROQ_MODEL}")

    def _split_long_document(self, content: str, filename: str) -> List[str]:
        """
        Divide un documento largo en chunks manejables.

        Args:
            content: Contenido completo del documento
            filename: Nombre del archivo (para logging)

        Returns:
            Lista de chunks de texto
        """
        content_length = len(content)
        max_chunk_size = FAISS_Creator.MAX_CHARS_PER_CHUNK

        if content_length <= max_chunk_size:
            return [content]

        logger.info(f"üìè Documento largo detectado: {content_length:,} chars")
        logger.info(f"üî™ Dividiendo en chunks de ~{max_chunk_size:,} chars")

        chunks = []
        overlap = FAISS_Creator.CHUNK_OVERLAP_CHARS
        start = 0
        chunk_num = 1

        while start < content_length:
            end = start + max_chunk_size

            # Si no es el √∫ltimo chunk, intentar cortar en un punto natural
            if end < content_length:
                # Buscar un salto de l√≠nea cerca del final
                search_start = max(start, end - 1000)
                newline_pos = content.rfind('\n\n', search_start, end)

                if newline_pos != -1 and newline_pos > start:
                    end = newline_pos
                else:
                    # Si no hay doble salto, buscar salto simple
                    newline_pos = content.rfind('\n', search_start, end)
                    if newline_pos != -1 and newline_pos > start:
                        end = newline_pos
                    else:
                        # Buscar un punto
                        period_pos = content.rfind('. ', search_start, end)
                        if period_pos != -1 and period_pos > start:
                            end = period_pos + 1

            chunk = content[start:end].strip()
            if chunk:
                chunks.append(chunk)
                logger.info(f"  ‚úÇÔ∏è Chunk {chunk_num}: {len(chunk):,} chars")
                chunk_num += 1

            # Mover start con overlap para mantener contexto
            start = end - overlap if end < content_length else content_length

        logger.info(f"‚úÖ Documento dividido en {len(chunks)} chunks")
        return chunks

    def _summarize_chunk(
            self,
            chunk: str,
            chunk_num: int,
            total_chunks: int,
            category: str,
            filename: str
    ) -> str:
            """
            Resume un chunk individual de un documento.

            Args:
                chunk: Texto del chunk
                chunk_num: N√∫mero del chunk actual
                total_chunks: Total de chunks
                category: Categor√≠a del documento
                filename: Nombre del archivo original

            Returns:
                Resumen del chunk
            """
            system_prompt = self._get_summary_prompt(category)

            # Prompt espec√≠fico para chunks
            if total_chunks > 1:
                user_message = f"""Este es el CHUNK {chunk_num} de {total_chunks} del documento: {filename}

    IMPORTANTE: 
    - Resume SOLO este fragmento manteniendo toda la informaci√≥n relevante
    - NO menciones que es un fragmento o chunk
    - Mant√©n el formato estructurado seg√∫n las instrucciones del system prompt
    - Si este chunk termina abruptamente, no inventes conclusiones

    CONTENIDO DEL CHUNK {chunk_num}/{total_chunks}:
    {chunk}

    Genera el resumen de este fragmento siguiendo las instrucciones del system prompt."""
            else:
                user_message = f"""Documento a resumir: {filename}

    CONTENIDO:
    {chunk}

    Genera el resumen siguiendo las instrucciones del system prompt."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]

            try:
                response = self.llm.invoke(messages)
                return response.content.strip()
            except Exception as e:
                logger.error(f"‚ùå Error resumiendo chunk {chunk_num}: {e}")
                return f"[ERROR EN CHUNK {chunk_num}] {str(e)}"

    def _merge_chunk_summaries(self, chunk_summaries: List[str], category: str, filename: str) -> str:
        """
        Combina res√∫menes de chunks en un resumen final cohesivo usando merge jer√°rquico.

        Args:
            chunk_summaries: Lista de res√∫menes de chunks
            category: Categor√≠a del documento
            filename: Nombre del archivo original

        Returns:
            Resumen final combinado
        """
        if len(chunk_summaries) == 1:
            return chunk_summaries[0]

        logger.info(f"üîó Combinando {len(chunk_summaries)} res√∫menes parciales...")

        # üÜï Si hay muchos chunks, usar merge jer√°rquico (combinar en grupos peque√±os)
        if len(chunk_summaries) > 4:
            logger.info(f"üìö Usando merge jer√°rquico para {len(chunk_summaries)} res√∫menes...")
            return self._hierarchical_merge(chunk_summaries, category, filename)

        # Para 2-4 chunks, merge directo (como antes)
        combined_text = "\n\n---SEPARADOR DE SECCI√ìN---\n\n".join(
            f"RESUMEN PARTE {i + 1}:\n{summary}"
            for i, summary in enumerate(chunk_summaries)
        )

        merge_prompt = f"""Eres un experto sintetizador de informaci√≥n.

    Te voy a dar {len(chunk_summaries)} res√∫menes parciales de diferentes secciones del mismo documento sobre {category}.

    Tu tarea es:
    1. COMBINAR toda la informaci√≥n de los {len(chunk_summaries)} res√∫menes en un √öNICO resumen cohesivo
    2. ELIMINAR redundancias y repeticiones
    3. ORGANIZAR la informaci√≥n de forma l√≥gica y estructurada
    4. MANTENER toda la informaci√≥n importante de cada secci√≥n
    5. Usar el mismo formato estructurado que se pidi√≥ originalmente
    6. NO mencionar que esto viene de m√∫ltiples partes

    IMPORTANTE:
    - El resultado debe leerse como un resumen √∫nico y natural
    - Mant√©n TODO el detalle relevante de cada parte
    - Si hay informaci√≥n complementaria entre partes, int√©grala
    - El resumen final puede ser extenso para mantener toda la informaci√≥n

    Documento original: {filename}

    RES√öMENES PARCIALES A COMBINAR:

    {combined_text}

    Genera el resumen final √∫nico y cohesivo:"""

        try:
            response = self.llm.invoke([HumanMessage(content=merge_prompt)])
            merged_summary = response.content.strip()
            logger.info(f"‚úÖ Resumen final combinado: {len(merged_summary):,} chars")
            return merged_summary
        except Exception as e:
            logger.error(f"‚ùå Error combinando res√∫menes: {e}")
            # Fallback: concatenar con separadores
            logger.warning("‚ö†Ô∏è Usando fallback: concatenando res√∫menes con separadores")
            return "\n\n".join(chunk_summaries)

    def _hierarchical_merge(self, summaries: List[str], category: str, filename: str) -> str:
        """
        Combina res√∫menes usando estrategia jer√°rquica (merge en grupos de 2).
        Trunca res√∫menes largos para garantizar que quepan en el l√≠mite de tokens.

        Args:
            summaries: Lista de res√∫menes a combinar
            category: Categor√≠a del documento
            filename: Nombre del archivo

        Returns:
            Resumen final combinado
        """
        import time

        GROUP_SIZE = 2
        MAX_CHARS_PER_SUMMARY = 10000  # ~2,500 tokens por resumen, 5,000 total + prompt = <6,000
        current_level = summaries.copy()
        level = 1

        while len(current_level) > 1:
            logger.info(f"üîÑ Nivel {level} de merge: {len(current_level)} res√∫menes -> grupos de {GROUP_SIZE}")
            next_level = []

            # Dividir en grupos de GROUP_SIZE
            for i in range(0, len(current_level), GROUP_SIZE):
                group = current_level[i:i + GROUP_SIZE]

                if len(group) == 1:
                    # Si solo queda 1, pasarlo directamente al siguiente nivel
                    next_level.append(group[0])
                    continue

                logger.info(f"   üîó Combinando grupo {i // GROUP_SIZE + 1}: {len(group)} res√∫menes")

                # ‚úÖ TRUNCAR res√∫menes si son muy largos
                truncated_group = []
                for idx, summary in enumerate(group):
                    if len(summary) > MAX_CHARS_PER_SUMMARY:
                        truncated = summary[:MAX_CHARS_PER_SUMMARY] + "\n\n[... contenido truncado para merge ...]"
                        logger.warning(
                            f"      ‚ö†Ô∏è Resumen {idx + 1} truncado: {len(summary):,} -> {len(truncated):,} chars")
                        truncated_group.append(truncated)
                    else:
                        truncated_group.append(summary)

                # Combinar este grupo
                combined_text = "\n\n---SEPARADOR---\n\n".join(
                    f"PARTE {j + 1}:\n{summary}"
                    for j, summary in enumerate(truncated_group)
                )

                # ‚úÖ PROMPT M√ÅS CORTO para ahorrar tokens
                merge_prompt = f"""Combina estos {len(truncated_group)} res√∫menes en uno solo:

    REGLAS:
    - Integra toda la informaci√≥n
    - Elimina redundancias
    - Mant√©n formato estructurado

    {combined_text}

    Resumen combinado:"""

                try:
                    response = self.llm.invoke([HumanMessage(content=merge_prompt)])
                    merged = response.content.strip()
                    next_level.append(merged)
                    logger.info(f"   ‚úÖ Grupo combinado: {len(merged):,} chars")

                    # Pausa para evitar rate limits
                    time.sleep(2)

                except Exception as e:
                    logger.error(f"   ‚ùå Error en grupo {i // GROUP_SIZE + 1}: {e}")
                    logger.warning(f"   ‚ö†Ô∏è Usando fallback: concatenando directamente")
                    # Fallback: concatenar este grupo
                    next_level.append("\n\n".join(truncated_group))

            current_level = next_level
            level += 1

        logger.info(f"‚úÖ Merge jer√°rquico completo en {level - 1} niveles")
        return current_level[0]
    def _get_summary_prompt(
        self,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]
    ) -> str:
        """Retorna el prompt especializado seg√∫n la categor√≠a"""

        prompts = {
            "biografias_jugadores": """Eres un experto en f√∫tbol especializado en an√°lisis de jugadores.

Tu tarea es crear un resumen COMPLETO Y DETALLADO de la informaci√≥n sobre este jugador.

El resumen debe incluir:
1. **Datos b√°sicos**: Nombre completo, apodo, fecha de nacimiento, nacionalidad
2. **Informaci√≥n f√≠sica**: Altura, peso, posici√≥n principal, pierna dominante
3. **Carrera profesional**: Equipos donde ha jugado (con fechas), logros importantes
4. **Estad√≠sticas destacadas**: Goles, asistencias, t√≠tulos ganados
5. **Estilo de juego**: Fortalezas, caracter√≠sticas t√©cnicas, rol t√°ctico
6. **Datos contextuales**: Premios individuales, reconocimientos, curiosidades

IMPORTANTE:
- Mant√©n TODA la informaci√≥n relevante del documento original
- Usa formato estructurado con secciones claras
- No inventes datos, solo resume lo que est√° en el documento
- Si faltan datos, omite esa secci√≥n
- El resumen puede ser extenso (500-800 palabras) para mantener detalle

Responde SOLO con el resumen estructurado, sin pre√°mbulos.""",

            "informacion_equipos": """Eres un experto en historia y an√°lisis de clubes de f√∫tbol.

Tu tarea es crear un resumen COMPLETO Y DETALLADO de la informaci√≥n sobre este equipo.

El resumen debe incluir:
1. **Identidad del club**: Nombre completo, apodo, a√±o de fundaci√≥n, ciudad/pa√≠s
2. **Estadio**: Nombre, capacidad, caracter√≠sticas
3. **Colores y escudo**: Descripci√≥n de la identidad visual
4. **Historia**: Momentos clave, eras doradas, evoluci√≥n del club
5. **Palmar√©s**: T√≠tulos nacionales, internacionales, otros logros
6. **Jugadores legendarios**: √çdolos hist√≥ricos y actuales
7. **Rivalidades**: Cl√°sicos y rivalidades importantes
8. **Datos actuales**: Entrenador, liga, situaci√≥n reciente

IMPORTANTE:
- Mant√©n TODA la informaci√≥n relevante del documento original
- Usa formato estructurado con secciones claras
- Preserva fechas, n√∫meros y datos espec√≠ficos
- El resumen puede ser extenso (600-1000 palabras) para mantener contexto
- No inventes informaci√≥n

Responde SOLO con el resumen estructurado, sin pre√°mbulos.""",

            "competiciones_y_reglas": """Eres un experto en reglamentos y competiciones de f√∫tbol.

Tu tarea es crear un resumen COMPLETO Y DETALLADO de esta informaci√≥n sobre reglas o competencias.

El resumen debe incluir:

Para REGLAS/REGLAMENTOS:
1. **Regla o aspecto**: Qu√© regla o aspecto del juego se describe
2. **Descripci√≥n detallada**: Explicaci√≥n completa de c√≥mo funciona
3. **Casos especiales**: Excepciones, situaciones particulares
4. **Ejemplos**: Casos de aplicaci√≥n pr√°ctica
5. **Cambios recientes**: Si aplica, cambios en el reglamento

Para COMPETENCIAS:
1. **Nombre y tipo**: Nombre oficial, categor√≠a (liga, copa, etc.)
2. **Formato**: Sistema de competici√≥n, n√∫mero de equipos
3. **Historia**: A√±o de fundaci√≥n, datos hist√≥ricos relevantes
4. **Equipos participantes**: Principales clubes o selecciones
5. **Sistema de clasificaci√≥n**: C√≥mo se determina el ganador
6. **Premios**: T√≠tulos, clasificaciones a otras competencias
7. **R√©cords y estad√≠sticas**: Datos destacados

IMPORTANTE:
- Mant√©n precisi√≥n en reglas y formatos
- Preserva n√∫meros, fechas y datos espec√≠ficos
- Usa formato claro y estructurado
- El resumen puede ser extenso (500-900 palabras)
- No simplificar excesivamente las reglas

Responde SOLO con el resumen estructurado, sin pre√°mbulos."""
        }

        return prompts[category]

    def summarize_document(
            self,
            file_path: Path,
            category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"],
            output_file: Path,
            force: bool = False
    ) -> Tuple[str, bool]:
        """
        Resume un documento usando Groq LLM.

        Args:
            file_path: Ruta del archivo .txt a resumir
            category: Categor√≠a del documento
            output_file: Ruta donde se guardar√° el resumen
            force: Si es True, regenera el resumen aunque ya exista

        Returns:
            Tupla (resumen, was_skipped)
            - resumen: Texto del resumen generado o existente
            - was_skipped: True si se salt√≥ porque ya exist√≠a
        """

        # üÜï VERIFICAR SI YA EXISTE EL RESUMEN
        if output_file.exists() and not force:
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_summary = f.read()

                if existing_summary.strip() and not existing_summary.startswith("[ERROR"):
                    logger.info(f"‚è≠Ô∏è  SALTANDO: {file_path.name} (resumen ya existe: {len(existing_summary)} chars)")
                    return existing_summary, True
                else:
                    logger.warning(f"‚ö†Ô∏è  Resumen existente inv√°lido, regenerando: {file_path.name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Error leyendo resumen existente, regenerando: {e}")

        # Si llegamos aqu√≠, debemos generar el resumen
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            if not content.strip():
                logger.warning(f"‚ö†Ô∏è Archivo vac√≠o: {file_path.name}")
                return f"[DOCUMENTO VAC√çO] {file_path.name}", False

            logger.info(f"üìÑ Resumiendo: {file_path.name} ({len(content):,} chars)")

            # üÜï DIVIDIR DOCUMENTO LARGO EN CHUNKS SI ES NECESARIO
            chunks = self._split_long_document(content, file_path.name)

            if len(chunks) == 1:
                # Documento corto - proceso normal
                system_prompt = self._get_summary_prompt(category)

                user_message = f"""Documento a resumir: {file_path.name}

    CONTENIDO:
    {chunks[0]}

    Genera el resumen siguiendo las instrucciones del system prompt."""

                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_message)
                ]

                response = self.llm.invoke(messages)
                summary = response.content.strip()

                logger.info(f"‚úÖ Resumen generado: {len(summary):,} chars")
                return summary, False

            else:
                # Documento largo - resumir por chunks y combinar
                logger.info(f"üìö Documento largo: procesando {len(chunks)} chunks...")

                chunk_summaries = []
                for i, chunk in enumerate(chunks, 1):
                    logger.info(f"üîÑ Procesando chunk {i}/{len(chunks)}...")
                    chunk_summary = self._summarize_chunk(
                        chunk=chunk,
                        chunk_num=i,
                        total_chunks=len(chunks),
                        category=category,
                        filename=file_path.name
                    )
                    chunk_summaries.append(chunk_summary)

                # Combinar todos los res√∫menes de chunks
                logger.info(f"üîó Combinando {len(chunk_summaries)} res√∫menes parciales...")
                final_summary = self._merge_chunk_summaries(
                    chunk_summaries=chunk_summaries,
                    category=category,
                    filename=file_path.name
                )

                logger.info(f"‚úÖ Resumen final generado: {len(final_summary):,} chars")
                return final_summary, False

        except Exception as e:
            logger.exception(f"‚ùå Error resumiendo {file_path.name}: {e}")
            return f"[ERROR AL RESUMIR] {file_path.name}: {str(e)}", False

    def summarize_category(
        self,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"],
        force: bool = False
    ) -> Dict[str, int]:
        """
        Resume todos los documentos de una categor√≠a.

        Args:
            category: Categor√≠a a procesar
            force: Si es True, regenera todos los res√∫menes aunque ya existan

        Returns:
            Estad√≠sticas: total, processed, skipped, errors
        """

        source_dirs = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_DIR,
            "informacion_equipos": FAISS_Creator.TEAMS_DIR,
            "competiciones_y_reglas": FAISS_Creator.RULES_DIR
        }

        output_dirs = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_SUMMARIES,
            "informacion_equipos": FAISS_Creator.TEAMS_SUMMARIES,
            "competiciones_y_reglas": FAISS_Creator.RULES_SUMMARIES
        }

        source_dir = source_dirs[category]
        output_dir = output_dirs[category]

        txt_files = list(source_dir.glob("*.txt"))

        if not txt_files:
            logger.warning(f"‚ö†Ô∏è No se encontraron archivos .txt en {source_dir}")
            return {"total": 0, "processed": 0, "skipped": 0, "errors": 0}

        logger.info(f"üîÑ Procesando {len(txt_files)} archivos de categor√≠a '{category}'")

        if force:
            logger.warning(f"üî• MODO FORCE activado: regenerando TODOS los res√∫menes")

        processed = 0
        skipped = 0
        errors = 0

        for txt_file in txt_files:
            try:
                output_file = output_dir / f"{txt_file.stem}_summary.txt"

                summary, was_skipped = self.summarize_document(
                    txt_file,
                    category,
                    output_file,
                    force=force
                )

                if was_skipped:
                    skipped += 1
                else:
                    # Solo escribimos si generamos un resumen nuevo
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(summary)
                    logger.info(f"üíæ Guardado: {output_file.name}")
                    processed += 1

            except Exception as e:
                logger.exception(f"‚ùå Error procesando {txt_file.name}: {e}")
                errors += 1

        return {
            "total": len(txt_files),
            "processed": processed,
            "skipped": skipped,
            "errors": errors
        }

    def summarize_all(self, force: bool = False) -> Dict[str, Dict[str, int]]:
        """
        Resume todos los documentos de todas las categor√≠as.

        Args:
            force: Si es True, regenera todos los res√∫menes aunque ya existan
        """
        logger.info("üöÄ Iniciando resumen de todos los documentos")

        if force:
            logger.warning("üî• MODO FORCE: Se regenerar√°n TODOS los res√∫menes")

        results = {}
        categories = ["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]

        for category in categories:
            logger.info(f"\n{'='*60}")
            logger.info(f"CATEGOR√çA: {category.upper()}")
            logger.info(f"{'='*60}")

            stats = self.summarize_category(category, force=force)
            results[category] = stats

            logger.info(
                f"‚úÖ {category}: "
                f"{stats['processed']} generados, "
                f"{stats['skipped']} saltados, "
                f"{stats['errors']} errores"
            )

        # Mostrar resumen total
        total_processed = sum(r['processed'] for r in results.values())
        total_skipped = sum(r['skipped'] for r in results.values())
        total_errors = sum(r['errors'] for r in results.values())

        logger.info(f"\n{'='*60}")
        logger.info("üìä RESUMEN TOTAL")
        logger.info(f"{'='*60}")
        logger.info(f"‚úÖ Generados: {total_processed}")
        logger.info(f"‚è≠Ô∏è  Saltados: {total_skipped}")
        logger.info(f"‚ùå Errores: {total_errors}")

        return results


# ============================================================================
# TOOL 2: FAISS VECTOR DB CREATOR (CON HUGGINGFACE EMBEDDINGS - GRATUITO)
# ============================================================================

class FAISSVectorDBCreator:
    """
    Tool que crea 3 bases de datos vectoriales FAISS separadas.

    Caracter√≠sticas:
    - Crea una FAISS DB por categor√≠a (jugadores, equipos, reglas)
    - Usa embeddings de HuggingFace (GRATUITOS, multiling√ºes)
    - Chunking inteligente de documentos
    - Metadata para tracking de fuente
    - NO requiere API keys adicionales
    """

    def __init__(self):
        logger.info("[FAISSVectorDBCreator] Inicializando embeddings de HuggingFace...")
        logger.info("‚è≥ Primera ejecuci√≥n: descargando modelo (puede tardar 1-2 min)...")

        # HuggingFace Embeddings - COMPLETAMENTE GRATUITO
        # Modelo multiling√ºe optimizado para espa√±ol
        self.embeddings = HuggingFaceEmbeddings(
            model_name=FAISS_Creator.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},  # Usa CPU (cambia a 'cuda' si tienes GPU)
            encode_kwargs={'normalize_embeddings': True}  # Mejora la similitud
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=FAISS_Creator.CHUNK_SIZE,
            chunk_overlap=FAISS_Creator.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        logger.info(f"‚úÖ Embeddings inicializados: {FAISS_Creator.EMBEDDING_MODEL}")

    def _load_documents_from_summaries(
        self,
        summaries_dir: Path,
        category: str
    ) -> List[Document]:
        """Carga archivos de res√∫menes en Documents de LangChain."""

        documents = []
        txt_files = list(summaries_dir.glob("*_summary.txt"))

        if not txt_files:
            logger.warning(f"‚ö†Ô∏è No se encontraron res√∫menes en {summaries_dir}")
            return documents

        logger.info(f"üìÇ Cargando {len(txt_files)} res√∫menes de {category}")

        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                if not content.strip():
                    logger.warning(f"‚ö†Ô∏è Resumen vac√≠o: {txt_file.name}")
                    continue

                doc = Document(
                    page_content=content,
                    metadata={
                        "source": txt_file.name,
                        "category": category,
                        "original_file": txt_file.stem.replace("_summary", "")
                    }
                )

                documents.append(doc)
                logger.info(f"‚úÖ Cargado: {txt_file.name} ({len(content)} chars)")

            except Exception as e:
                logger.exception(f"‚ùå Error cargando {txt_file.name}: {e}")

        return documents

    def create_vectordb_for_category(
        self,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]
    ) -> Dict[str, any]:
        """Crea una base de datos vectorial FAISS para una categor√≠a."""

        logger.info(f"\n{'='*60}")
        logger.info(f"CREANDO VECTORDB: {category.upper()}")
        logger.info(f"{'='*60}")

        summaries_dirs = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_SUMMARIES,
            "informacion_equipos": FAISS_Creator.TEAMS_SUMMARIES,
            "competiciones_y_reglas": FAISS_Creator.RULES_SUMMARIES
        }

        output_paths = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_VECTORDB,
            "informacion_equipos": FAISS_Creator.TEAMS_VECTORDB,
            "competiciones_y_reglas": FAISS_Creator.RULES_VECTORDB
        }

        summaries_dir = summaries_dirs[category]
        output_path = output_paths[category]

        try:
            documents = self._load_documents_from_summaries(summaries_dir, category)

            if not documents:
                logger.warning(f"‚ö†Ô∏è No hay documentos para procesar en {category}")
                return {
                    "category": category,
                    "documents_loaded": 0,
                    "chunks_created": 0,
                    "success": False
                }

            logger.info(f"üìä {len(documents)} documentos cargados")

            logger.info("‚úÇÔ∏è Aplicando chunking...")
            chunks = self.text_splitter.split_documents(documents)
            logger.info(f"‚úÖ {len(chunks)} chunks creados")

            logger.info("üß† Generando embeddings y creando FAISS index...")
            vectorstore = FAISS.from_documents(chunks, self.embeddings)

            logger.info(f"üíæ Guardando vectorstore en {output_path}")
            vectorstore.save_local(str(output_path))

            logger.info(f"‚úÖ VectorDB creada exitosamente para {category}")

            return {
                "category": category,
                "documents_loaded": len(documents),
                "chunks_created": len(chunks),
                "output_path": str(output_path),
                "success": True
            }

        except Exception as e:
            logger.exception(f"‚ùå Error creando vectorDB para {category}: {e}")
            return {
                "category": category,
                "documents_loaded": 0,
                "chunks_created": 0,
                "success": False,
                "error": str(e)
            }

    def create_all_vectordbs(self) -> Dict[str, Dict[str, any]]:
        """Crea las 3 bases de datos vectoriales."""

        logger.info("\n" + "="*70)
        logger.info("üöÄ INICIANDO CREACI√ìN DE BASES DE DATOS VECTORIALES")
        logger.info("="*70 + "\n")

        results = {}
        categories = ["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]

        for category in categories:
            result = self.create_vectordb_for_category(category)
            results[category] = result

        logger.info("\n" + "="*70)
        logger.info("üìä RESUMEN FINAL")
        logger.info("="*70)

        for category, stats in results.items():
            if stats['success']:
                logger.info(f"‚úÖ {category}: {stats['documents_loaded']} docs, {stats['chunks_created']} chunks")
            else:
                logger.info(f"‚ùå {category}: FALL√ì")

        return results


# ============================================================================
# FAISS_Creator AGENT (Orquestador)
# ============================================================================

class FAISS_CreatorAgent:
    """
    Agente orquestador que ejecuta el pipeline completo de construcci√≥n de conocimiento.

    Pipeline:
    1. Crear estructura de directorios
    2. Resumir todos los documentos (Tool 1 - Groq) - SALTA EXISTENTES
    3. Crear bases de datos vectoriales (Tool 2 - HuggingFace embeddings gratuitos)
    4. Generar reporte de ejecuci√≥n

    SOLO REQUIERE: GROQ_API_KEY
    """

    def __init__(self):
        logger.info("ü§ñ Inicializando FAISS Creator Agent")
        self.summarizer = DocumentSummarizer()
        self.vectordb_creator = FAISSVectorDBCreator()
        FAISS_Creator.create_directories()

    def run_full_pipeline(self, force_summaries: bool = False) -> Dict[str, any]:
        """
        Ejecuta el pipeline completo de construcci√≥n de conocimiento.

        Args:
            force_summaries: Si es True, regenera todos los res√∫menes
        """

        logger.info("\n" + "üî•"*35)
        logger.info("üöÄ INICIANDO PIPELINE COMPLETO DE FAISS CREATOR")
        logger.info("üî•"*35 + "\n")

        report = {
            "timestamp": None,
            "summary_results": None,
            "vectordb_results": None,
            "success": False
        }

        try:
            logger.info("\nüìù PASO 1: RESUMIENDO DOCUMENTOS")
            logger.info("-" * 70)
            summary_results = self.summarizer.summarize_all(force=force_summaries)
            report['summary_results'] = summary_results

            logger.info("\nüß† PASO 2: CREANDO BASES DE DATOS VECTORIALES")
            logger.info("-" * 70)
            vectordb_results = self.vectordb_creator.create_all_vectordbs()
            report['vectordb_results'] = vectordb_results

            all_success = all(
                result['success'] for result in vectordb_results.values()
            )

            report['success'] = all_success

            from datetime import datetime
            report['timestamp'] = datetime.now().isoformat()

            report_path = FAISS_Creator.VECTOR_DBS_DIR / "build_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            logger.info(f"\nüíæ Reporte guardado en: {report_path}")

            if all_success:
                logger.info("\n" + "‚úÖ"*35)
                logger.info("üéâ PIPELINE COMPLETADO EXITOSAMENTE")
                logger.info("‚úÖ"*35 + "\n")
            else:
                logger.warning("\n" + "‚ö†Ô∏è"*35)
                logger.warning("‚ö†Ô∏è PIPELINE COMPLETADO CON ERRORES")
                logger.warning("‚ö†Ô∏è"*35 + "\n")

            return report

        except Exception as e:
            logger.exception(f"‚ùå Error cr√≠tico en el pipeline: {e}")
            report['success'] = False
            report['error'] = str(e)
            return report

    def run_summary_only(self, force: bool = False) -> Dict[str, Dict[str, int]]:
        """
        Ejecuta solo el paso de res√∫menes

        Args:
            force: Si es True, regenera todos los res√∫menes aunque ya existan
        """
        logger.info("üìù Ejecutando SOLO res√∫menes de documentos")
        if force:
            logger.warning("üî• MODO FORCE: regenerando TODOS los res√∫menes")
        return self.summarizer.summarize_all(force=force)

    def run_vectordb_only(self) -> Dict[str, Dict[str, any]]:
        """Ejecuta solo el paso de creaci√≥n de vector DBs"""
        logger.info("üß† Ejecutando SOLO creaci√≥n de vector databases")
        return self.vectordb_creator.create_all_vectordbs()


# ============================================================================
# SCRIPT DE EJECUCI√ìN
# ============================================================================

def main():
    """Punto de entrada principal"""

    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë           FAIS CREATOR AGENT v2.1                         ‚ïë
    ‚ïë           (Con detecci√≥n de res√∫menes existentes)              ‚ïë
    ‚ïë           Solo requiere GROQ_API_KEY                           ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    print("\nOpciones:")
    print("1. üî• Ejecutar pipeline completo (res√∫menes + vector DBs)")
    print("2. üìù Solo resumir documentos (salta existentes)")
    print("3. üîÑ Regenerar TODOS los res√∫menes (force)")
    print("4. üß† Solo crear vector databases (requiere res√∫menes previos)")
    print("5. üîç Diagn√≥stico de configuraci√≥n")
    print("6. ‚ùå Salir")

    choice = input("\nSelecciona una opci√≥n (1-6): ").strip()

    if choice == "6":
        print("üëã Saliendo...")
        return

    if choice == "5":
        run_diagnostics()
        return

    agent = FAISS_CreatorAgent()

    if choice == "1":
        agent.run_full_pipeline(force_summaries=False)
    elif choice == "2":
        agent.run_summary_only(force=False)
    elif choice == "3":
        confirm = input("‚ö†Ô∏è  ¬øSeguro que quieres regenerar TODOS los res√∫menes? (s/n): ").strip().lower()
        if confirm == 's':
            agent.run_summary_only(force=True)
        else:
            print("‚ùå Operaci√≥n cancelada")
            return
    elif choice == "4":
        agent.run_vectordb_only()
    else:
        print("‚ùå Opci√≥n inv√°lida")
        return

    print("\n‚úÖ Proceso finalizado. Revisa los logs para m√°s detalles.")


def run_diagnostics():
    """Ejecuta un diagn√≥stico completo del sistema"""
    print("\n" + "="*70)
    print("üîç DIAGN√ìSTICO DEL SISTEMA")
    print("="*70 + "\n")

    # 1. Verificar ubicaci√≥n del script
    script_path = Path(__file__).resolve()
    print(f"üìç Ubicaci√≥n del script:")
    print(f"   {script_path}")
    print(f"   Carpeta: {script_path.parent}\n")

    # 2. Verificar directorio de ejecuci√≥n
    print(f"üìÅ Directorio de ejecuci√≥n actual:")
    print(f"   {Path.cwd()}\n")

    # 3. Buscar .env en app/
    app_dir = script_path.parent.parent  # app/
    env_path = app_dir / ".env"

    print(f"üîç Buscando .env en app/:")
    print(f"   Ruta esperada: {env_path}")

    if env_path.exists():
        print(f"   ‚úÖ Encontrado\n")

        # Leer y mostrar contenido (sin mostrar la key completa)
        try:
            with open(env_path, 'r') as f:
                content = f.read()
                if "GROQ_API_KEY" in content:
                    print(f"   ‚úÖ Contiene GROQ_API_KEY")

                    # Intentar cargar
                    load_dotenv(dotenv_path=env_path)
                    groq_key = os.getenv("GROQ_API_KEY")

                    if groq_key:
                        print(f"   ‚úÖ Key cargada correctamente")
                        print(f"   üìä Longitud: {len(groq_key)} caracteres")
                        print(f"   üîí Primeros 15 chars: {groq_key[:15]}...")
                        print(f"   üîí √öltimos 5 chars: ...{groq_key[-5:]}")
                    else:
                        print(f"   ‚ùå Key NO se pudo cargar")
                else:
                    print(f"   ‚ùå NO contiene GROQ_API_KEY")
                    print(f"   üìù Contenido actual:")
                    print(f"   {content[:200]}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error leyendo archivo: {e}")
    else:
        print(f"   ‚ùå NO existe\n")
        print(f"üí° Crea el archivo en: {env_path}")
        print(f"üìù Con este contenido:")
        print(f"   GROQ_API_KEY=gsk_tu_key_real_aqui\n")

    # 4. Verificar estructura de data/
    print(f"\nüìÇ Verificando estructura de data/:")
    data_dir = app_dir / "data"  # app/data/
    print(f"   Ubicaci√≥n: {data_dir}")

    if data_dir.exists():
        print(f"   ‚úÖ data/ existe\n")

        required_dirs = [
            "biografias_jugadores",
            "informacion_equipos",
            "competiciones_y_reglas"
        ]

        for dir_name in required_dirs:
            dir_path = data_dir / dir_name
            if dir_path.exists():
                txt_files = list(dir_path.glob("*.txt"))
                txt_count = len(txt_files)
                print(f"   ‚úÖ {dir_name}/ - {txt_count} archivos .txt")
                if txt_count > 0:
                    print(f"      Ejemplos: {', '.join([f.name for f in txt_files[:3]])}")
            else:
                print(f"   ‚ùå {dir_name}/ NO existe")
                print(f"      Ruta esperada: {dir_path}")
    else:
        print(f"   ‚ùå data/ NO existe")
        print(f"   üí° Debes crear: {data_dir}")
        print(f"   Con las subcarpetas:")
        print(f"      - biografias_jugadores/")
        print(f"      - informacion_equipos/")
        print(f"      - competiciones_y_reglas/")

    # 5. Verificar res√∫menes existentes
    print(f"\nüìã Verificando res√∫menes existentes:")
    summaries_dir = data_dir / "summaries"

    if summaries_dir.exists():
        print(f"   ‚úÖ summaries/ existe")

        summary_dirs = {
            "biografias_jugadores": summaries_dir / "biografias_jugadores",
            "informacion_equipos": summaries_dir / "informacion_equipos",
            "competiciones_y_reglas": summaries_dir / "competiciones_y_reglas"
        }

        for cat_name, cat_path in summary_dirs.items():
            if cat_path.exists():
                summary_files = list(cat_path.glob("*_summary.txt"))
                print(f"   ‚úÖ {cat_name}: {len(summary_files)} res√∫menes")
            else:
                print(f"   ‚ö†Ô∏è {cat_name}: carpeta no existe")
    else:
        print(f"   ‚ö†Ô∏è summaries/ NO existe (se crear√° al ejecutar)")

    # 6. Test de conexi√≥n con Groq (si la key est√° disponible)
    groq_key = os.getenv("GROQ_API_KEY")
    if groq_key:
        print(f"\nüß™ Probando conexi√≥n con Groq API...")
        try:
            from langchain_groq import ChatGroq
            from langchain_core.messages import HumanMessage

            llm = ChatGroq(
                temperature=0,
                model_name="openai/gpt-oss-20b",
                api_key=groq_key
            )

            response = llm.invoke([HumanMessage(content="Di 'OK' si funcion√≥")])
            print(f"   ‚úÖ Conexi√≥n exitosa!")
            print(f"   üìù Respuesta: {response.content}")
        except Exception as e:
            print(f"   ‚ùå Error de conexi√≥n: {e}")

    print("\n" + "="*70)
    print("‚úÖ Diagn√≥stico completado")
    print("="*70 + "\n")

    # Resumen de acciones necesarias
    print("üìã RESUMEN DE ACCIONES NECESARIAS:")

    actions_needed = []

    if not env_path.exists() or not os.getenv("GROQ_API_KEY"):
        actions_needed.append(f"1. Crear/verificar {env_path} con GROQ_API_KEY")

    if not data_dir.exists():
        actions_needed.append(f"2. Crear carpeta {data_dir}")

    for dir_name in ["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]:
        dir_path = data_dir / dir_name
        if not dir_path.exists():
            actions_needed.append(f"3. Crear carpeta {dir_path}")

    if actions_needed:
        for action in actions_needed:
            print(f"   ‚ö†Ô∏è {action}")
    else:
        print(f"   ‚úÖ Todo est√° configurado correctamente")

    print()


if __name__ == "__main__":
    main()