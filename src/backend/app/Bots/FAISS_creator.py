"""
FAISS_Creator Agent - Preprocesamiento de Documentos y Creaci√≥n de Vector DBs
==================================================================================

Este agente se ejecuta de forma independiente para:
1. Resumir documentos .txt usando Groq LLM (SALTA RES√öMENES EXISTENTES)
2. Crear 3 bases de datos vectoriales FAISS categorizadas usando embeddings de HuggingFace (gratuitos)
3. Preparar el conocimiento para el sistema principal

NO forma parte del flujo conversacional, solo prepara los datos.
SOLO REQUIERE: GROQ_API_KEY

v2.1 - Detecta y salta autom√°ticamente res√∫menes ya existentes
"""

import os
import json
from pathlib import Path
from typing import List, Dict, Literal, Tuple
from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_huggingface import HuggingFaceEmbeddings  # Embeddings GRATUITOS
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from dotenv import load_dotenv
import logging

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Cargar variables de entorno
current_dir = Path(__file__).resolve().parent  # bots/
app_dir = current_dir.parent                    # app/

# El .env est√° en app/ directamente
env_path = app_dir / ".env"

if env_path.exists():
    load_dotenv(dotenv_path=env_path)
    logger.info(f"üìÅ .env encontrado en: {env_path}")
else:
    logger.error(f"‚ùå .env NO encontrado en: {env_path}")

# Validar SOLO Groq API key
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

if not GROQ_API_KEY:
    error_msg = f"""
‚ùå ERROR: GROQ_API_KEY no encontrada.

üîß Ubicaciones verificadas:
   - {env_path}

üìù Tu .env debe contener:
   GROQ_API_KEY=gsk_tu_key_real_aqui

üåê Obt√©n tu key en: https://console.groq.com/keys
"""
    raise ValueError(error_msg)

logger.info(f"‚úÖ GROQ_API_KEY cargada (primeros 10 chars: {GROQ_API_KEY[:10]}...)")


# ============================================================================
# CONFIGURACI√ìN
# ============================================================================

class FAISS_Creator:
    """Configuraci√≥n centralizada del FAISS_Creator"""

    SCRIPT_PATH = Path(__file__).resolve()
    SCRIPT_DIR = SCRIPT_PATH.parent           # bots/
    APP_DIR = SCRIPT_DIR.parent               # app/
    DATA_DIR = APP_DIR / "data"               # app/data/

    SUMMARIES_DIR = DATA_DIR / "summaries"
    VECTOR_DBS_DIR = DATA_DIR / "vector_stores"

    # Subdirectorios de documentos originales
    PLAYERS_DIR = DATA_DIR / "biografias_jugadores"
    TEAMS_DIR = DATA_DIR / "informacion_equipos"
    RULES_DIR = DATA_DIR / "competiciones_y_reglas"

    # Subdirectorios de summaries
    PLAYERS_SUMMARIES = SUMMARIES_DIR / "biografias_jugadores"
    TEAMS_SUMMARIES = SUMMARIES_DIR / "informacion_equipos"
    RULES_SUMMARIES = SUMMARIES_DIR / "competiciones_y_reglas"

    # Vector DBs paths
    PLAYERS_VECTORDB = VECTOR_DBS_DIR / "jugadores_faiss"
    TEAMS_VECTORDB = VECTOR_DBS_DIR / "equipos_faiss"
    RULES_VECTORDB = VECTOR_DBS_DIR / "reglas_faiss"

    # Modelos

    GROQ_MODEL = "llama-3.1-8b-instant" #Para textos cortos

    # Embeddings de HuggingFace (GRATUITOS, no requieren API key)
    EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

    # Chunking
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200

    @classmethod
    def create_directories(cls):
        """Crea solo los directorios que no existen (summaries y vector_stores)"""
        dirs = [
            cls.SUMMARIES_DIR,
            cls.VECTOR_DBS_DIR,
            cls.PLAYERS_SUMMARIES,
            cls.TEAMS_SUMMARIES,
            cls.RULES_SUMMARIES,
        ]
        for d in dirs:
            d.mkdir(parents=True, exist_ok=True)
        logger.info("‚úÖ Estructura de directorios de salida creada")

        # Verificar que existan los directorios de entrada
        logger.info(f"\nüìÇ Verificando directorios de entrada:")
        logger.info(f"   Data dir: {cls.DATA_DIR}")

        missing_dirs = []
        for name, dir_path in [
            ("biografias_jugadores", cls.PLAYERS_DIR),
            ("informacion_equipos", cls.TEAMS_DIR),
            ("competiciones_y_reglas", cls.RULES_DIR)
        ]:
            if dir_path.exists():
                txt_count = len(list(dir_path.glob("*.txt")))
                logger.info(f"   ‚úÖ {name}: {txt_count} archivos .txt")
            else:
                logger.warning(f"   ‚ùå {name}: NO EXISTE - {dir_path}")
                missing_dirs.append(str(dir_path))

        if missing_dirs:
            logger.warning(f"\n‚ö†Ô∏è ADVERTENCIA: Faltan carpetas con datos")
            logger.warning(f"Aseg√∫rate de que existan estas carpetas:")
            for d in missing_dirs:
                logger.warning(f"  - {d}")


# ============================================================================
# TOOL 1: DOCUMENT SUMMARIZER (CON DETECCI√ìN DE RES√öMENES EXISTENTES)
# ============================================================================

class DocumentSummarizer:
    """
    Tool que resume documentos .txt usando Groq LLM.

    üÜï v2.1 Caracter√≠sticas:
    - Detecta autom√°ticamente res√∫menes existentes y los salta
    - Solo procesa documentos nuevos o faltantes
    - Modo 'force' para regenerar todos los res√∫menes
    - Estad√≠sticas detalladas: generados, saltados, errores
    """

    def __init__(self):
        self.llm = ChatGroq(
            temperature=0.3,
            model_name=FAISS_Creator.GROQ_MODEL,
            api_key=GROQ_API_KEY
        )
        logger.info(f"[DocumentSummarizer] Inicializado con {FAISS_Creator.GROQ_MODEL}")

    def _get_summary_prompt(
        self,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]
    ) -> str:
        """Retorna el prompt especializado seg√∫n la categor√≠a"""

        prompts = {
            "biografias_jugadores": """Eres un experto en f√∫tbol especializado en an√°lisis de jugadores.

Tu tarea es crear un resumen COMPLETO Y DETALLADO de la informaci√≥n sobre este jugador.

El resumen debe incluir:
1. **Datos b√°sicos**: Nombre completo, apodo, fecha de nacimiento, nacionalidad
2. **Informaci√≥n f√≠sica**: Altura, peso, posici√≥n principal, pierna dominante
3. **Carrera profesional**: Equipos donde ha jugado (con fechas), logros importantes
4. **Estad√≠sticas destacadas**: Goles, asistencias, t√≠tulos ganados
5. **Estilo de juego**: Fortalezas, caracter√≠sticas t√©cnicas, rol t√°ctico
6. **Datos contextuales**: Premios individuales, reconocimientos, curiosidades

IMPORTANTE:
- Mant√©n TODA la informaci√≥n relevante del documento original
- Usa formato estructurado con secciones claras
- No inventes datos, solo resume lo que est√° en el documento
- Si faltan datos, omite esa secci√≥n
- El resumen puede ser extenso (500-800 palabras) para mantener detalle

Responde SOLO con el resumen estructurado, sin pre√°mbulos.""",

            "informacion_equipos": """Eres un experto en historia y an√°lisis de clubes de f√∫tbol.

Tu tarea es crear un resumen COMPLETO Y DETALLADO de la informaci√≥n sobre este equipo.

El resumen debe incluir:
1. **Identidad del club**: Nombre completo, apodo, a√±o de fundaci√≥n, ciudad/pa√≠s
2. **Estadio**: Nombre, capacidad, caracter√≠sticas
3. **Colores y escudo**: Descripci√≥n de la identidad visual
4. **Historia**: Momentos clave, eras doradas, evoluci√≥n del club
5. **Palmar√©s**: T√≠tulos nacionales, internacionales, otros logros
6. **Jugadores legendarios**: √çdolos hist√≥ricos y actuales
7. **Rivalidades**: Cl√°sicos y rivalidades importantes
8. **Datos actuales**: Entrenador, liga, situaci√≥n reciente

IMPORTANTE:
- Mant√©n TODA la informaci√≥n relevante del documento original
- Usa formato estructurado con secciones claras
- Preserva fechas, n√∫meros y datos espec√≠ficos
- El resumen puede ser extenso (600-1000 palabras) para mantener contexto
- No inventes informaci√≥n

Responde SOLO con el resumen estructurado, sin pre√°mbulos.""",

            "competiciones_y_reglas": """Eres un experto en reglamentos y competiciones de f√∫tbol.

Tu tarea es crear un resumen COMPLETO Y DETALLADO de esta informaci√≥n sobre reglas o competencias.

El resumen debe incluir:

Para REGLAS/REGLAMENTOS:
1. **Regla o aspecto**: Qu√© regla o aspecto del juego se describe
2. **Descripci√≥n detallada**: Explicaci√≥n completa de c√≥mo funciona
3. **Casos especiales**: Excepciones, situaciones particulares
4. **Ejemplos**: Casos de aplicaci√≥n pr√°ctica
5. **Cambios recientes**: Si aplica, cambios en el reglamento

Para COMPETENCIAS:
1. **Nombre y tipo**: Nombre oficial, categor√≠a (liga, copa, etc.)
2. **Formato**: Sistema de competici√≥n, n√∫mero de equipos
3. **Historia**: A√±o de fundaci√≥n, datos hist√≥ricos relevantes
4. **Equipos participantes**: Principales clubes o selecciones
5. **Sistema de clasificaci√≥n**: C√≥mo se determina el ganador
6. **Premios**: T√≠tulos, clasificaciones a otras competencias
7. **R√©cords y estad√≠sticas**: Datos destacados

IMPORTANTE:
- Mant√©n precisi√≥n en reglas y formatos
- Preserva n√∫meros, fechas y datos espec√≠ficos
- Usa formato claro y estructurado
- El resumen puede ser extenso (500-900 palabras)
- No simplificar excesivamente las reglas

Responde SOLO con el resumen estructurado, sin pre√°mbulos."""
        }

        return prompts[category]

    def summarize_document(
        self,
        file_path: Path,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"],
        output_file: Path,
        force: bool = False
    ) -> Tuple[str, bool]:
        """
        Resume un documento usando Groq LLM.

        Args:
            file_path: Ruta del archivo .txt a resumir
            category: Categor√≠a del documento
            output_file: Ruta donde se guardar√° el resumen
            force: Si es True, regenera el resumen aunque ya exista

        Returns:
            Tupla (resumen, was_skipped)
            - resumen: Texto del resumen generado o existente
            - was_skipped: True si se salt√≥ porque ya exist√≠a
        """

        # üÜï VERIFICAR SI YA EXISTE EL RESUMEN
        if output_file.exists() and not force:
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_summary = f.read()

                if existing_summary.strip() and not existing_summary.startswith("[ERROR"):
                    logger.info(f"‚è≠Ô∏è  SALTANDO: {file_path.name} (resumen ya existe: {len(existing_summary)} chars)")
                    return existing_summary, True
                else:
                    logger.warning(f"‚ö†Ô∏è  Resumen existente inv√°lido, regenerando: {file_path.name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Error leyendo resumen existente, regenerando: {e}")

        # Si llegamos aqu√≠, debemos generar el resumen
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            if not content.strip():
                logger.warning(f"‚ö†Ô∏è Archivo vac√≠o: {file_path.name}")
                return f"[DOCUMENTO VAC√çO] {file_path.name}", False

            logger.info(f"üìÑ Resumiendo: {file_path.name} ({len(content)} chars)")

            system_prompt = self._get_summary_prompt(category)

            user_message = f"""Documento a resumir: {file_path.name}

CONTENIDO:
{content}

Genera el resumen siguiendo las instrucciones del system prompt."""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]

            response = self.llm.invoke(messages)
            summary = response.content.strip()

            logger.info(f"‚úÖ Resumen generado: {len(summary)} chars")
            return summary, False

        except Exception as e:
            logger.exception(f"‚ùå Error resumiendo {file_path.name}: {e}")
            return f"[ERROR AL RESUMIR] {file_path.name}: {str(e)}", False

    def summarize_category(
        self,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"],
        force: bool = False
    ) -> Dict[str, int]:
        """
        Resume todos los documentos de una categor√≠a.

        Args:
            category: Categor√≠a a procesar
            force: Si es True, regenera todos los res√∫menes aunque ya existan

        Returns:
            Estad√≠sticas: total, processed, skipped, errors
        """

        source_dirs = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_DIR,
            "informacion_equipos": FAISS_Creator.TEAMS_DIR,
            "competiciones_y_reglas": FAISS_Creator.RULES_DIR
        }

        output_dirs = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_SUMMARIES,
            "informacion_equipos": FAISS_Creator.TEAMS_SUMMARIES,
            "competiciones_y_reglas": FAISS_Creator.RULES_SUMMARIES
        }

        source_dir = source_dirs[category]
        output_dir = output_dirs[category]

        txt_files = list(source_dir.glob("*.txt"))

        if not txt_files:
            logger.warning(f"‚ö†Ô∏è No se encontraron archivos .txt en {source_dir}")
            return {"total": 0, "processed": 0, "skipped": 0, "errors": 0}

        logger.info(f"üîÑ Procesando {len(txt_files)} archivos de categor√≠a '{category}'")

        if force:
            logger.warning(f"üî• MODO FORCE activado: regenerando TODOS los res√∫menes")

        processed = 0
        skipped = 0
        errors = 0

        for txt_file in txt_files:
            try:
                output_file = output_dir / f"{txt_file.stem}_summary.txt"

                summary, was_skipped = self.summarize_document(
                    txt_file,
                    category,
                    output_file,
                    force=force
                )

                if was_skipped:
                    skipped += 1
                else:
                    # Solo escribimos si generamos un resumen nuevo
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(summary)
                    logger.info(f"üíæ Guardado: {output_file.name}")
                    processed += 1

            except Exception as e:
                logger.exception(f"‚ùå Error procesando {txt_file.name}: {e}")
                errors += 1

        return {
            "total": len(txt_files),
            "processed": processed,
            "skipped": skipped,
            "errors": errors
        }

    def summarize_all(self, force: bool = False) -> Dict[str, Dict[str, int]]:
        """
        Resume todos los documentos de todas las categor√≠as.

        Args:
            force: Si es True, regenera todos los res√∫menes aunque ya existan
        """
        logger.info("üöÄ Iniciando resumen de todos los documentos")

        if force:
            logger.warning("üî• MODO FORCE: Se regenerar√°n TODOS los res√∫menes")

        results = {}
        categories = ["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]

        for category in categories:
            logger.info(f"\n{'='*60}")
            logger.info(f"CATEGOR√çA: {category.upper()}")
            logger.info(f"{'='*60}")

            stats = self.summarize_category(category, force=force)
            results[category] = stats

            logger.info(
                f"‚úÖ {category}: "
                f"{stats['processed']} generados, "
                f"{stats['skipped']} saltados, "
                f"{stats['errors']} errores"
            )

        # Mostrar resumen total
        total_processed = sum(r['processed'] for r in results.values())
        total_skipped = sum(r['skipped'] for r in results.values())
        total_errors = sum(r['errors'] for r in results.values())

        logger.info(f"\n{'='*60}")
        logger.info("üìä RESUMEN TOTAL")
        logger.info(f"{'='*60}")
        logger.info(f"‚úÖ Generados: {total_processed}")
        logger.info(f"‚è≠Ô∏è  Saltados: {total_skipped}")
        logger.info(f"‚ùå Errores: {total_errors}")

        return results


# ============================================================================
# TOOL 2: FAISS VECTOR DB CREATOR (CON HUGGINGFACE EMBEDDINGS - GRATUITO)
# ============================================================================

class FAISSVectorDBCreator:
    """
    Tool que crea 3 bases de datos vectoriales FAISS separadas.

    Caracter√≠sticas:
    - Crea una FAISS DB por categor√≠a (jugadores, equipos, reglas)
    - Usa embeddings de HuggingFace (GRATUITOS, multiling√ºes)
    - Chunking inteligente de documentos
    - Metadata para tracking de fuente
    - NO requiere API keys adicionales
    """

    def __init__(self):
        logger.info("[FAISSVectorDBCreator] Inicializando embeddings de HuggingFace...")
        logger.info("‚è≥ Primera ejecuci√≥n: descargando modelo (puede tardar 1-2 min)...")

        # HuggingFace Embeddings - COMPLETAMENTE GRATUITO
        # Modelo multiling√ºe optimizado para espa√±ol
        self.embeddings = HuggingFaceEmbeddings(
            model_name=FAISS_Creator.EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'},  # Usa CPU (cambia a 'cuda' si tienes GPU)
            encode_kwargs={'normalize_embeddings': True}  # Mejora la similitud
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=FAISS_Creator.CHUNK_SIZE,
            chunk_overlap=FAISS_Creator.CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

        logger.info(f"‚úÖ Embeddings inicializados: {FAISS_Creator.EMBEDDING_MODEL}")

    def _load_documents_from_summaries(
        self,
        summaries_dir: Path,
        category: str
    ) -> List[Document]:
        """Carga archivos de res√∫menes en Documents de LangChain."""

        documents = []
        txt_files = list(summaries_dir.glob("*_summary.txt"))

        if not txt_files:
            logger.warning(f"‚ö†Ô∏è No se encontraron res√∫menes en {summaries_dir}")
            return documents

        logger.info(f"üìÇ Cargando {len(txt_files)} res√∫menes de {category}")

        for txt_file in txt_files:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                if not content.strip():
                    logger.warning(f"‚ö†Ô∏è Resumen vac√≠o: {txt_file.name}")
                    continue

                doc = Document(
                    page_content=content,
                    metadata={
                        "source": txt_file.name,
                        "category": category,
                        "original_file": txt_file.stem.replace("_summary", "")
                    }
                )

                documents.append(doc)
                logger.info(f"‚úÖ Cargado: {txt_file.name} ({len(content)} chars)")

            except Exception as e:
                logger.exception(f"‚ùå Error cargando {txt_file.name}: {e}")

        return documents

    def create_vectordb_for_category(
        self,
        category: Literal["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]
    ) -> Dict[str, any]:
        """Crea una base de datos vectorial FAISS para una categor√≠a."""

        logger.info(f"\n{'='*60}")
        logger.info(f"CREANDO VECTORDB: {category.upper()}")
        logger.info(f"{'='*60}")

        summaries_dirs = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_SUMMARIES,
            "informacion_equipos": FAISS_Creator.TEAMS_SUMMARIES,
            "competiciones_y_reglas": FAISS_Creator.RULES_SUMMARIES
        }

        output_paths = {
            "biografias_jugadores": FAISS_Creator.PLAYERS_VECTORDB,
            "informacion_equipos": FAISS_Creator.TEAMS_VECTORDB,
            "competiciones_y_reglas": FAISS_Creator.RULES_VECTORDB
        }

        summaries_dir = summaries_dirs[category]
        output_path = output_paths[category]

        try:
            documents = self._load_documents_from_summaries(summaries_dir, category)

            if not documents:
                logger.warning(f"‚ö†Ô∏è No hay documentos para procesar en {category}")
                return {
                    "category": category,
                    "documents_loaded": 0,
                    "chunks_created": 0,
                    "success": False
                }

            logger.info(f"üìä {len(documents)} documentos cargados")

            logger.info("‚úÇÔ∏è Aplicando chunking...")
            chunks = self.text_splitter.split_documents(documents)
            logger.info(f"‚úÖ {len(chunks)} chunks creados")

            logger.info("üß† Generando embeddings y creando FAISS index...")
            vectorstore = FAISS.from_documents(chunks, self.embeddings)

            logger.info(f"üíæ Guardando vectorstore en {output_path}")
            vectorstore.save_local(str(output_path))

            logger.info(f"‚úÖ VectorDB creada exitosamente para {category}")

            return {
                "category": category,
                "documents_loaded": len(documents),
                "chunks_created": len(chunks),
                "output_path": str(output_path),
                "success": True
            }

        except Exception as e:
            logger.exception(f"‚ùå Error creando vectorDB para {category}: {e}")
            return {
                "category": category,
                "documents_loaded": 0,
                "chunks_created": 0,
                "success": False,
                "error": str(e)
            }

    def create_all_vectordbs(self) -> Dict[str, Dict[str, any]]:
        """Crea las 3 bases de datos vectoriales."""

        logger.info("\n" + "="*70)
        logger.info("üöÄ INICIANDO CREACI√ìN DE BASES DE DATOS VECTORIALES")
        logger.info("="*70 + "\n")

        results = {}
        categories = ["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]

        for category in categories:
            result = self.create_vectordb_for_category(category)
            results[category] = result

        logger.info("\n" + "="*70)
        logger.info("üìä RESUMEN FINAL")
        logger.info("="*70)

        for category, stats in results.items():
            if stats['success']:
                logger.info(f"‚úÖ {category}: {stats['documents_loaded']} docs, {stats['chunks_created']} chunks")
            else:
                logger.info(f"‚ùå {category}: FALL√ì")

        return results


# ============================================================================
# FAISS_Creator AGENT (Orquestador)
# ============================================================================

class FAISS_CreatorAgent:
    """
    Agente orquestador que ejecuta el pipeline completo de construcci√≥n de conocimiento.

    Pipeline:
    1. Crear estructura de directorios
    2. Resumir todos los documentos (Tool 1 - Groq) - SALTA EXISTENTES
    3. Crear bases de datos vectoriales (Tool 2 - HuggingFace embeddings gratuitos)
    4. Generar reporte de ejecuci√≥n

    SOLO REQUIERE: GROQ_API_KEY
    """

    def __init__(self):
        logger.info("ü§ñ Inicializando FAISS Creator Agent")
        self.summarizer = DocumentSummarizer()
        self.vectordb_creator = FAISSVectorDBCreator()
        FAISS_Creator.create_directories()

    def run_full_pipeline(self, force_summaries: bool = False) -> Dict[str, any]:
        """
        Ejecuta el pipeline completo de construcci√≥n de conocimiento.

        Args:
            force_summaries: Si es True, regenera todos los res√∫menes
        """

        logger.info("\n" + "üî•"*35)
        logger.info("üöÄ INICIANDO PIPELINE COMPLETO DE FAISS CREATOR")
        logger.info("üî•"*35 + "\n")

        report = {
            "timestamp": None,
            "summary_results": None,
            "vectordb_results": None,
            "success": False
        }

        try:
            logger.info("\nüìù PASO 1: RESUMIENDO DOCUMENTOS")
            logger.info("-" * 70)
            summary_results = self.summarizer.summarize_all(force=force_summaries)
            report['summary_results'] = summary_results

            logger.info("\nüß† PASO 2: CREANDO BASES DE DATOS VECTORIALES")
            logger.info("-" * 70)
            vectordb_results = self.vectordb_creator.create_all_vectordbs()
            report['vectordb_results'] = vectordb_results

            all_success = all(
                result['success'] for result in vectordb_results.values()
            )

            report['success'] = all_success

            from datetime import datetime
            report['timestamp'] = datetime.now().isoformat()

            report_path = FAISS_Creator.VECTOR_DBS_DIR / "build_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            logger.info(f"\nüíæ Reporte guardado en: {report_path}")

            if all_success:
                logger.info("\n" + "‚úÖ"*35)
                logger.info("üéâ PIPELINE COMPLETADO EXITOSAMENTE")
                logger.info("‚úÖ"*35 + "\n")
            else:
                logger.warning("\n" + "‚ö†Ô∏è"*35)
                logger.warning("‚ö†Ô∏è PIPELINE COMPLETADO CON ERRORES")
                logger.warning("‚ö†Ô∏è"*35 + "\n")

            return report

        except Exception as e:
            logger.exception(f"‚ùå Error cr√≠tico en el pipeline: {e}")
            report['success'] = False
            report['error'] = str(e)
            return report

    def run_summary_only(self, force: bool = False) -> Dict[str, Dict[str, int]]:
        """
        Ejecuta solo el paso de res√∫menes

        Args:
            force: Si es True, regenera todos los res√∫menes aunque ya existan
        """
        logger.info("üìù Ejecutando SOLO res√∫menes de documentos")
        if force:
            logger.warning("üî• MODO FORCE: regenerando TODOS los res√∫menes")
        return self.summarizer.summarize_all(force=force)

    def run_vectordb_only(self) -> Dict[str, Dict[str, any]]:
        """Ejecuta solo el paso de creaci√≥n de vector DBs"""
        logger.info("üß† Ejecutando SOLO creaci√≥n de vector databases")
        return self.vectordb_creator.create_all_vectordbs()


# ============================================================================
# SCRIPT DE EJECUCI√ìN
# ============================================================================

def main():
    """Punto de entrada principal"""

    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë           FAIS CREATOR AGENT v2.1                         ‚ïë
    ‚ïë           (Con detecci√≥n de res√∫menes existentes)              ‚ïë
    ‚ïë           Solo requiere GROQ_API_KEY                           ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    print("\nOpciones:")
    print("1. üî• Ejecutar pipeline completo (res√∫menes + vector DBs)")
    print("2. üìù Solo resumir documentos (salta existentes)")
    print("3. üîÑ Regenerar TODOS los res√∫menes (force)")
    print("4. üß† Solo crear vector databases (requiere res√∫menes previos)")
    print("5. üîç Diagn√≥stico de configuraci√≥n")
    print("6. ‚ùå Salir")

    choice = input("\nSelecciona una opci√≥n (1-6): ").strip()

    if choice == "6":
        print("üëã Saliendo...")
        return

    if choice == "5":
        run_diagnostics()
        return

    agent = FAISS_CreatorAgent()

    if choice == "1":
        agent.run_full_pipeline(force_summaries=False)
    elif choice == "2":
        agent.run_summary_only(force=False)
    elif choice == "3":
        confirm = input("‚ö†Ô∏è  ¬øSeguro que quieres regenerar TODOS los res√∫menes? (s/n): ").strip().lower()
        if confirm == 's':
            agent.run_summary_only(force=True)
        else:
            print("‚ùå Operaci√≥n cancelada")
            return
    elif choice == "4":
        agent.run_vectordb_only()
    else:
        print("‚ùå Opci√≥n inv√°lida")
        return

    print("\n‚úÖ Proceso finalizado. Revisa los logs para m√°s detalles.")


def run_diagnostics():
    """Ejecuta un diagn√≥stico completo del sistema"""
    print("\n" + "="*70)
    print("üîç DIAGN√ìSTICO DEL SISTEMA")
    print("="*70 + "\n")

    # 1. Verificar ubicaci√≥n del script
    script_path = Path(__file__).resolve()
    print(f"üìç Ubicaci√≥n del script:")
    print(f"   {script_path}")
    print(f"   Carpeta: {script_path.parent}\n")

    # 2. Verificar directorio de ejecuci√≥n
    print(f"üìÅ Directorio de ejecuci√≥n actual:")
    print(f"   {Path.cwd()}\n")

    # 3. Buscar .env en app/
    app_dir = script_path.parent.parent  # app/
    env_path = app_dir / ".env"

    print(f"üîç Buscando .env en app/:")
    print(f"   Ruta esperada: {env_path}")

    if env_path.exists():
        print(f"   ‚úÖ Encontrado\n")

        # Leer y mostrar contenido (sin mostrar la key completa)
        try:
            with open(env_path, 'r') as f:
                content = f.read()
                if "GROQ_API_KEY" in content:
                    print(f"   ‚úÖ Contiene GROQ_API_KEY")

                    # Intentar cargar
                    load_dotenv(dotenv_path=env_path)
                    groq_key = os.getenv("GROQ_API_KEY")

                    if groq_key:
                        print(f"   ‚úÖ Key cargada correctamente")
                        print(f"   üìä Longitud: {len(groq_key)} caracteres")
                        print(f"   üîí Primeros 15 chars: {groq_key[:15]}...")
                        print(f"   üîí √öltimos 5 chars: ...{groq_key[-5:]}")
                    else:
                        print(f"   ‚ùå Key NO se pudo cargar")
                else:
                    print(f"   ‚ùå NO contiene GROQ_API_KEY")
                    print(f"   üìù Contenido actual:")
                    print(f"   {content[:200]}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error leyendo archivo: {e}")
    else:
        print(f"   ‚ùå NO existe\n")
        print(f"üí° Crea el archivo en: {env_path}")
        print(f"üìù Con este contenido:")
        print(f"   GROQ_API_KEY=gsk_tu_key_real_aqui\n")

    # 4. Verificar estructura de data/
    print(f"\nüìÇ Verificando estructura de data/:")
    data_dir = app_dir / "data"  # app/data/
    print(f"   Ubicaci√≥n: {data_dir}")

    if data_dir.exists():
        print(f"   ‚úÖ data/ existe\n")

        required_dirs = [
            "biografias_jugadores",
            "informacion_equipos",
            "competiciones_y_reglas"
        ]

        for dir_name in required_dirs:
            dir_path = data_dir / dir_name
            if dir_path.exists():
                txt_files = list(dir_path.glob("*.txt"))
                txt_count = len(txt_files)
                print(f"   ‚úÖ {dir_name}/ - {txt_count} archivos .txt")
                if txt_count > 0:
                    print(f"      Ejemplos: {', '.join([f.name for f in txt_files[:3]])}")
            else:
                print(f"   ‚ùå {dir_name}/ NO existe")
                print(f"      Ruta esperada: {dir_path}")
    else:
        print(f"   ‚ùå data/ NO existe")
        print(f"   üí° Debes crear: {data_dir}")
        print(f"   Con las subcarpetas:")
        print(f"      - biografias_jugadores/")
        print(f"      - informacion_equipos/")
        print(f"      - competiciones_y_reglas/")

    # 5. Verificar res√∫menes existentes
    print(f"\nüìã Verificando res√∫menes existentes:")
    summaries_dir = data_dir / "summaries"

    if summaries_dir.exists():
        print(f"   ‚úÖ summaries/ existe")

        summary_dirs = {
            "biografias_jugadores": summaries_dir / "biografias_jugadores",
            "informacion_equipos": summaries_dir / "informacion_equipos",
            "competiciones_y_reglas": summaries_dir / "competiciones_y_reglas"
        }

        for cat_name, cat_path in summary_dirs.items():
            if cat_path.exists():
                summary_files = list(cat_path.glob("*_summary.txt"))
                print(f"   ‚úÖ {cat_name}: {len(summary_files)} res√∫menes")
            else:
                print(f"   ‚ö†Ô∏è {cat_name}: carpeta no existe")
    else:
        print(f"   ‚ö†Ô∏è summaries/ NO existe (se crear√° al ejecutar)")

    # 6. Test de conexi√≥n con Groq (si la key est√° disponible)
    groq_key = os.getenv("GROQ_API_KEY")
    if groq_key:
        print(f"\nüß™ Probando conexi√≥n con Groq API...")
        try:
            from langchain_groq import ChatGroq
            from langchain_core.messages import HumanMessage

            llm = ChatGroq(
                temperature=0,
                model_name="llama-3.1-8b-instant",
                api_key=groq_key
            )

            response = llm.invoke([HumanMessage(content="Di 'OK' si funcion√≥")])
            print(f"   ‚úÖ Conexi√≥n exitosa!")
            print(f"   üìù Respuesta: {response.content}")
        except Exception as e:
            print(f"   ‚ùå Error de conexi√≥n: {e}")

    print("\n" + "="*70)
    print("‚úÖ Diagn√≥stico completado")
    print("="*70 + "\n")

    # Resumen de acciones necesarias
    print("üìã RESUMEN DE ACCIONES NECESARIAS:")

    actions_needed = []

    if not env_path.exists() or not os.getenv("GROQ_API_KEY"):
        actions_needed.append(f"1. Crear/verificar {env_path} con GROQ_API_KEY")

    if not data_dir.exists():
        actions_needed.append(f"2. Crear carpeta {data_dir}")

    for dir_name in ["biografias_jugadores", "informacion_equipos", "competiciones_y_reglas"]:
        dir_path = data_dir / dir_name
        if not dir_path.exists():
            actions_needed.append(f"3. Crear carpeta {dir_path}")

    if actions_needed:
        for action in actions_needed:
            print(f"   ‚ö†Ô∏è {action}")
    else:
        print(f"   ‚úÖ Todo est√° configurado correctamente")

    print()


if __name__ == "__main__":
    main()